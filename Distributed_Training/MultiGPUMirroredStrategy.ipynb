{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MultiGPUMirroredStrategy.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U7yHQV9LjgT2"},"source":["# Multi-GPU Mirrored Strategy\n","\n","In this notebook we'll go on how to set up a Multi-GPU Mirrored Strategy.\n","\n","**Note:** \n"," \n","- One device is sufficient for helping you understand the these distribution strategies."]},{"cell_type":"code","metadata":{"id":"k0VOdqKP2NEz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622441975637,"user_tz":-180,"elapsed":650138,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"791f0ace-3d40-4f75-ef87-dc8e6acf2942"},"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","# Note that it generally has a minimum of 8 cores, but if your GPU has\n","# less, you need to set this. In this case one of my GPUs has 4 cores\n","os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\n","\n","# If the list of devices is not specified in the\n","# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n","# If you have *different* GPUs in your system, you probably have to set up cross_device_ops like this\n","strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n","print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n","\n","\n","# Get the data\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","\n","# Adding a dimension to the array -> new shape == (28, 28, 1)\n","# We are doing this because the first layer in our model is a convolutional\n","# layer and it requires a 4D input (batch_size, height, width, channels).\n","# batch_size dimension will be added later on.\n","train_images = train_images[..., None]\n","test_images = test_images[..., None]\n","\n","# Normalize the images to [0, 1] range.\n","train_images = train_images / np.float32(255)\n","test_images = test_images / np.float32(255)\n","\n","# Batch the input data\n","BUFFER_SIZE = len(train_images)\n","BATCH_SIZE_PER_REPLICA = 64\n","GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","\n","# Create Datasets from the batches\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n","\n","# Create Distributed Datasets from the datasets\n","train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n","\n","# Create the model architecture\n","def create_model():\n","  model = tf.keras.Sequential([\n","      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n","      tf.keras.layers.MaxPooling2D(),\n","      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n","      tf.keras.layers.MaxPooling2D(),\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dense(64, activation='relu'),\n","      tf.keras.layers.Dense(10)\n","    ])\n","  return model\n","\n","# Instead of model.compile, we're going to do custom training, so let's do that\n","# within a strategy scope\n","with strategy.scope():\n","    # We will use sparse categorical crossentropy as always. But, instead of having the loss function\n","    # manage the map reduce across GPUs for us, we'll do it ourselves with a simple algorithm.\n","    # Remember -- the map reduce is how the losses get aggregated\n","    # Set reduction to `none` so we can do the reduction afterwards and divide by global batch size.\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def compute_loss(labels, predictions):\n","        # Compute Loss uses the loss object to compute the loss\n","        # Notice that per_example_loss will have an entry per GPU\n","        # so in this case there'll be 2 -- i.e. the loss for each replica\n","        per_example_loss = loss_object(labels, predictions)\n","        # You can print it to see it -- you'll get output like this:\n","        # Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(48,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n","        # Tensor(\"replica_1/sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(48,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n","        # Note in particular that replica_0 isn't named in the weighted_loss -- the first is unnamed, the second is replica_1 etc\n","        print(per_example_loss)\n","        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n","\n","    # We'll just reduce by getting the average of the losses\n","    test_loss = tf.keras.metrics.Mean(name='test_loss')\n","\n","    # Accuracy on train and test will be SparseCategoricalAccuracy\n","    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","    # Optimizer will be Adam\n","    optimizer = tf.keras.optimizers.Adam()\n","\n","    # Create the model within the scope\n","    model = create_model()\n","\n","\n","###########################\n","# Training Steps Functions\n","###########################\n","\n","# `run` replicates the provided computation and runs it\n","# with the distributed input.\n","@tf.function\n","def distributed_train_step(dataset_inputs):\n","  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n","  #tf.print(per_replica_losses.values)\n","  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n","\n","def train_step(inputs):\n","  images, labels = inputs\n","  with tf.GradientTape() as tape:\n","    predictions = model(images, training=True)\n","    loss = compute_loss(labels, predictions)\n","\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_accuracy.update_state(labels, predictions)\n","  return loss\n","\n","#######################\n","# Test Steps Functions\n","#######################\n","@tf.function\n","def distributed_test_step(dataset_inputs):\n","  return strategy.run(test_step, args=(dataset_inputs,))\n","\n","def test_step(inputs):\n","  images, labels = inputs\n","\n","  predictions = model(images, training=False)\n","  t_loss = loss_object(labels, predictions)\n","\n","  test_loss.update_state(t_loss)\n","  test_accuracy.update_state(labels, predictions)\n","\n","\n","###############\n","# TRAINING LOOP\n","###############\n","\n","EPOCHS = 10\n","for epoch in range(EPOCHS):\n","  # Do Training\n","  total_loss = 0.0\n","  num_batches = 0\n","  for batch in train_dist_dataset:\n","    total_loss += distributed_train_step(batch)\n","    num_batches += 1\n","  train_loss = total_loss / num_batches\n","\n","  # Do Testing\n","  for batch in test_dist_dataset:\n","    distributed_test_step(batch)\n","\n","  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\n","\n","  print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n","\n","  test_loss.reset_states()\n","  train_accuracy.reset_states()\n","  test_accuracy.reset_states()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n","INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n","Number of devices: 1\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n","Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n","Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n","Epoch 1, Loss: 0.4957454800605774, Accuracy: 81.98833465576172, Test Loss: 0.40418094396591187, Test Accuracy: 85.0\n","Epoch 2, Loss: 0.3301312029361725, Accuracy: 88.07833099365234, Test Loss: 0.32400766015052795, Test Accuracy: 88.3800048828125\n","Epoch 3, Loss: 0.2871343493461609, Accuracy: 89.46666717529297, Test Loss: 0.30357444286346436, Test Accuracy: 88.9800033569336\n","Epoch 4, Loss: 0.2567587196826935, Accuracy: 90.52166748046875, Test Loss: 0.2814948558807373, Test Accuracy: 90.05000305175781\n","Epoch 5, Loss: 0.23447208106517792, Accuracy: 91.3550033569336, Test Loss: 0.27174821496009827, Test Accuracy: 90.13999938964844\n","Epoch 6, Loss: 0.2139267772436142, Accuracy: 92.09667205810547, Test Loss: 0.2662512958049774, Test Accuracy: 90.25\n","Epoch 7, Loss: 0.19689497351646423, Accuracy: 92.59166717529297, Test Loss: 0.25823888182640076, Test Accuracy: 90.66000366210938\n","Epoch 8, Loss: 0.18021932244300842, Accuracy: 93.29166412353516, Test Loss: 0.2641752064228058, Test Accuracy: 90.45999908447266\n","Epoch 9, Loss: 0.16560016572475433, Accuracy: 93.87333679199219, Test Loss: 0.2611444592475891, Test Accuracy: 90.80999755859375\n","Epoch 10, Loss: 0.15275558829307556, Accuracy: 94.36499786376953, Test Loss: 0.2758607566356659, Test Accuracy: 90.24000549316406\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7SlO8vzTjgUG"},"source":[""],"execution_count":null,"outputs":[]}]}