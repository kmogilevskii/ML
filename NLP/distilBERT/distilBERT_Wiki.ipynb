{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"distillBERT_Wiki.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNw+M2dngrCp/vu0ysHVcTy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4dd5ee21bce5462eb1b523155a5f201d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5cc55a9f7f124947a768efbd25c24382","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c97685ae1af74b8baffe451b50554092","IPY_MODEL_60a8fa2ab4b84997ad9dd053a7096c35"]}},"5cc55a9f7f124947a768efbd25c24382":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c97685ae1af74b8baffe451b50554092":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cb0a94b06e9d464eb37ddab67c078b0a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bc0e950a9d7e45f88ebc316df5f60faf"}},"60a8fa2ab4b84997ad9dd053a7096c35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4a351291105d496fa2a69643d9441905","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:02&lt;00:00, 78.3kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0880a43df21d49aa858aa2dc2790b90d"}},"cb0a94b06e9d464eb37ddab67c078b0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bc0e950a9d7e45f88ebc316df5f60faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a351291105d496fa2a69643d9441905":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0880a43df21d49aa858aa2dc2790b90d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"129b2e31a1c54762a09eee2bc721a3a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fd5659aed031409e88085b85a1e3d4eb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_394894c3ef6140769bf979200721c860","IPY_MODEL_507b02e96ba2403da0362386ebafbf9d"]}},"fd5659aed031409e88085b85a1e3d4eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"394894c3ef6140769bf979200721c860":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_40aa72c257234c0a94fb2f7e2c3bb4df","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8f039b538ba946baa0f7382c7f7cdb28"}},"507b02e96ba2403da0362386ebafbf9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5f365aeecacc406d837c24ba534b9e1a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:02&lt;00:00, 12.0B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_424b62d6eb4c497dae396a846fa416a6"}},"40aa72c257234c0a94fb2f7e2c3bb4df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8f039b538ba946baa0f7382c7f7cdb28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f365aeecacc406d837c24ba534b9e1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"424b62d6eb4c497dae396a846fa416a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30ef4643b489481e9068699dd80bfb5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_215ceffe2323419da92d02fa0ce60290","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a408ee126f3b41fd915e9b631ea36ff6","IPY_MODEL_2eef29965a6645318f8e701d3aa9f769"]}},"215ceffe2323419da92d02fa0ce60290":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a408ee126f3b41fd915e9b631ea36ff6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0bf176a7ffbc4f3c9037b15b6c4a1b88","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_689c9bc7efb9461ead60f70d8bb42d30"}},"2eef29965a6645318f8e701d3aa9f769":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7ea99488f6cf4bd4be479f24e9118182","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 502kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6b57bec9f9a946f993370c6dab8363ee"}},"0bf176a7ffbc4f3c9037b15b6c4a1b88":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"689c9bc7efb9461ead60f70d8bb42d30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ea99488f6cf4bd4be479f24e9118182":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6b57bec9f9a946f993370c6dab8363ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f3e20c3612db48d89fe963c31cb63af5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ee6a848c245a41f4906d7ada4871b0d7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fdc518c0d369479db0aeacbcd6caa6dc","IPY_MODEL_3bbe9898c9ef483da8201d0347d4e743"]}},"ee6a848c245a41f4906d7ada4871b0d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fdc518c0d369479db0aeacbcd6caa6dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e49ec6ea6c584962b80d530ac9ae4c60","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78e8806dc69b45ef8ab238be818e855d"}},"3bbe9898c9ef483da8201d0347d4e743":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_094bcbdec0ad435b9183e6ae72fe0b48","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442/442 [00:15&lt;00:00, 28.5B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5199e67f28d4457b944edefecedb905e"}},"e49ec6ea6c584962b80d530ac9ae4c60":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"78e8806dc69b45ef8ab238be818e855d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"094bcbdec0ad435b9183e6ae72fe0b48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5199e67f28d4457b944edefecedb905e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2517ac8066d427c81873d3493396bc9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8bd4d566b73940c78792d961474bb392","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_33026cdf929849839b090b3e5ad98b07","IPY_MODEL_c8ecd217beb64722912a7f755f7c9a30"]}},"8bd4d566b73940c78792d961474bb392":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33026cdf929849839b090b3e5ad98b07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b8afe63824b49ccb935c84572c1844f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b98705b96bce401aa7413e18feb4a1b5"}},"c8ecd217beb64722912a7f755f7c9a30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_11c176c81d184c36939b070d188c8310","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:04&lt;00:00, 62.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_89175c0046de4a91aabccbe365f9f4a2"}},"4b8afe63824b49ccb935c84572c1844f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b98705b96bce401aa7413e18feb4a1b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"11c176c81d184c36939b070d188c8310":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"89175c0046de4a91aabccbe365f9f4a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"V1S1ZP12093Y"},"source":["# Classifying Wikipedia Comments with BERT\n","\n","This Notebook will show you how to fine-tune BERT for *document* classification tasks using the Wikipedia Personal Attacks dataset as an example. As a bonus, in part 3, we'll also look briefly at how we can apply BERT to search for \"semantically similar\" comments in the dataset."]},{"cell_type":"code","metadata":{"id":"dxzmaSA11HVs","executionInfo":{"status":"ok","timestamp":1618744326713,"user_tz":-180,"elapsed":6053,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}}},"source":["import os\n","import time\n","import torch\n","import random\n","import urllib\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2q3x3L301Q0M","executionInfo":{"status":"ok","timestamp":1618744326721,"user_tz":-180,"elapsed":6023,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"a5f85746-87c2-4c9a-e02b-2c1fa4876daa"},"source":["if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","  print(f\"We have {torch.cuda.device_count()} GPUs available\")\n","  print(f\"Its name is {torch.cuda.get_device_name(0)}\")\n","else:\n","  device = torch.device('cpu')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["We have 1 GPUs available\n","Its name is Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U5tkwxTzstHl"},"source":["Next, let's install the [transformers](https://github.com/huggingface/transformers) package from Hugging Face which will give us a pytorch interface for working with BERT. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A92qK9lm2BZT","executionInfo":{"status":"ok","timestamp":1618744333374,"user_tz":-180,"elapsed":12662,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"484cddfb-f410-4b0c-a88e-b7de06252673"},"source":["!pip install transformers\n","from transformers import BertTokenizer\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import BertForSequenceClassification, AdamW"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 52.8MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 65.9MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=3203d266dbac463b6fbd4cd33aa2dd8a631d1ac26d18da9e84ee9b58127b4512\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d6wK-jXPtMb2"},"source":["## Download"]},{"cell_type":"code","metadata":{"id":"8x65tBL_suQw","executionInfo":{"status":"ok","timestamp":1618744350328,"user_tz":-180,"elapsed":29613,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}}},"source":["os.makedirs('data', exist_ok=True)\n","files = [\n","    ('./data/attack_annotated_comments.tsv',  'https://ndownloader.figshare.com/files/7554634'),\n","    ('./data/attack_annotations.tsv',         'https://ndownloader.figshare.com/files/7554637')   \n","]    \n","for (filename, url) in files:\n","  if not os.path.exists(filename):\n","    urllib.request.urlretrieve(url, filename)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ngr2P_Piuszr"},"source":["Let's investigate the dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"TFFuhEXKtwV7","executionInfo":{"status":"ok","timestamp":1618744351742,"user_tz":-180,"elapsed":31012,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"3f437aae-259d-47f7-9879-a06f27f69cd9"},"source":["comments = pd.read_csv('./data/attack_annotated_comments.tsv', sep='\\t', index_col=0)\n","annotations = pd.read_csv('./data/attack_annotations.tsv', sep='\\t')\n","comments.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>year</th>\n","      <th>logged_in</th>\n","      <th>ns</th>\n","      <th>sample</th>\n","      <th>split</th>\n","    </tr>\n","    <tr>\n","      <th>rev_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>37675</th>\n","      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n","      <td>2002</td>\n","      <td>False</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>44816</th>\n","      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n","      <td>2002</td>\n","      <td>False</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>49851</th>\n","      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n","      <td>2002</td>\n","      <td>False</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>89320</th>\n","      <td>Next, maybe you could work on being less cond...</td>\n","      <td>2002</td>\n","      <td>True</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>93890</th>\n","      <td>This page will need disambiguation.</td>\n","      <td>2002</td>\n","      <td>True</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  comment  year  ...  sample  split\n","rev_id                                                           ...               \n","37675   `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002  ...  random  train\n","44816   `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002  ...  random  train\n","49851   NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002  ...  random  train\n","89320    Next, maybe you could work on being less cond...  2002  ...  random    dev\n","93890                This page will need disambiguation.   2002  ...  random  train\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"ZJ2yp-S-ucmg","executionInfo":{"status":"ok","timestamp":1618744351745,"user_tz":-180,"elapsed":30999,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"dbd8f381-89e1-4719-8eb8-1538c9de1c61"},"source":["comments[['comment', 'split']].groupby('split').count()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","    </tr>\n","    <tr>\n","      <th>split</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>dev</th>\n","      <td>23160</td>\n","    </tr>\n","    <tr>\n","      <th>test</th>\n","      <td>23178</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>69526</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       comment\n","split         \n","dev      23160\n","test     23178\n","train    69526"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"8Q7KFaGXuny2","executionInfo":{"status":"ok","timestamp":1618744351747,"user_tz":-180,"elapsed":30983,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"5560c57c-71ba-40e1-e34b-d03297dfe05d"},"source":["annotations.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rev_id</th>\n","      <th>worker_id</th>\n","      <th>quoting_attack</th>\n","      <th>recipient_attack</th>\n","      <th>third_party_attack</th>\n","      <th>other_attack</th>\n","      <th>attack</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37675</td>\n","      <td>1362</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>37675</td>\n","      <td>2408</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>37675</td>\n","      <td>1493</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37675</td>\n","      <td>1439</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>37675</td>\n","      <td>170</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   rev_id  worker_id  quoting_attack  ...  third_party_attack  other_attack  attack\n","0   37675       1362             0.0  ...                 0.0           0.0     0.0\n","1   37675       2408             0.0  ...                 0.0           0.0     0.0\n","2   37675       1493             0.0  ...                 0.0           0.0     0.0\n","3   37675       1439             0.0  ...                 0.0           0.0     0.0\n","4   37675        170             0.0  ...                 0.0           0.0     0.0\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"oTv7_EtSu7Dv"},"source":["We will label text as attack if majority of the annotators did so. Each comment has unique `rev_id` and multiple rows with the same value of `rev_id` correspond to different decisions of annotators."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"2l5riMSFurvH","executionInfo":{"status":"ok","timestamp":1618744351749,"user_tz":-180,"elapsed":30968,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"89cffd18-8ee7-42e6-ecff-363437524d48"},"source":["labels = annotations.groupby('rev_id')['attack'].mean() > .5\n","comments['attack'] = labels\n","comments.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>year</th>\n","      <th>logged_in</th>\n","      <th>ns</th>\n","      <th>sample</th>\n","      <th>split</th>\n","      <th>attack</th>\n","    </tr>\n","    <tr>\n","      <th>rev_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>37675</th>\n","      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n","      <td>2002</td>\n","      <td>False</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>train</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>44816</th>\n","      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n","      <td>2002</td>\n","      <td>False</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>train</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>49851</th>\n","      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n","      <td>2002</td>\n","      <td>False</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>train</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>89320</th>\n","      <td>Next, maybe you could work on being less cond...</td>\n","      <td>2002</td>\n","      <td>True</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>dev</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>93890</th>\n","      <td>This page will need disambiguation.</td>\n","      <td>2002</td>\n","      <td>True</td>\n","      <td>article</td>\n","      <td>random</td>\n","      <td>train</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  comment  year  ...  split attack\n","rev_id                                                           ...              \n","37675   `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002  ...  train  False\n","44816   `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002  ...  train  False\n","49851   NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002  ...  train  False\n","89320    Next, maybe you could work on being less cond...  2002  ...    dev  False\n","93890                This page will need disambiguation.   2002  ...  train  False\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UMwzNqy_vweX","executionInfo":{"status":"ok","timestamp":1618744351750,"user_tz":-180,"elapsed":30954,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"53267df4-e98a-4fcb-bd68-43864f88e2da"},"source":["#removing unnecessary tokens\n","comments['comment'] = comments['comment'].apply(lambda x: x.replace('NEWLINE_TOKEN', ' '))\n","comments['comment'] = comments['comment'].apply(lambda x: x.replace('TAB_TOKEN', ' '))\n","#splitting dataset\n","train_comments = comments.query('split==\"train\"')\n","test_comments = comments.query('split==\"test\"')\n","print(f\"train_comments.shape: {train_comments.shape}, test_comments.shape: {test_comments.shape}\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["train_comments.shape: (69526, 7), test_comments.shape: (23178, 7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wk2-Yt7zw0--","executionInfo":{"status":"ok","timestamp":1618744351753,"user_tz":-180,"elapsed":30943,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"74ee31b0-81c3-4088-e1dc-ef77d3374e32"},"source":["num_of_comments = comments.shape[0]\n","num_of_attacks = comments.query('attack').shape[0]\n","attacks_to_all_ratio = num_of_attacks / num_of_comments\n","print(f\"Percent of attacks in our dataset is {attacks_to_all_ratio*100:.2f}%\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Percent of attacks in our dataset is 11.73%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RzzPSYfGyTq5"},"source":["We're working with highly skewed targets in our dataset. We should be careful interpreting accuracy."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJdX70yPyP6d","executionInfo":{"status":"ok","timestamp":1618744351754,"user_tz":-180,"elapsed":30923,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"3813a63f-0975-4a01-cc33-c6e98d441685"},"source":["print(f\"Always predicting 'not attack' will produce {(1 - attacks_to_all_ratio)*100:.2f}% accuracy\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Always predicting 'not attack' will produce 88.27% accuracy\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"17y03tiEzJQj"},"source":["# Time to load BERT Tokenizer."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["4dd5ee21bce5462eb1b523155a5f201d","5cc55a9f7f124947a768efbd25c24382","c97685ae1af74b8baffe451b50554092","60a8fa2ab4b84997ad9dd053a7096c35","cb0a94b06e9d464eb37ddab67c078b0a","bc0e950a9d7e45f88ebc316df5f60faf","4a351291105d496fa2a69643d9441905","0880a43df21d49aa858aa2dc2790b90d","129b2e31a1c54762a09eee2bc721a3a3","fd5659aed031409e88085b85a1e3d4eb","394894c3ef6140769bf979200721c860","507b02e96ba2403da0362386ebafbf9d","40aa72c257234c0a94fb2f7e2c3bb4df","8f039b538ba946baa0f7382c7f7cdb28","5f365aeecacc406d837c24ba534b9e1a","424b62d6eb4c497dae396a846fa416a6","30ef4643b489481e9068699dd80bfb5a","215ceffe2323419da92d02fa0ce60290","a408ee126f3b41fd915e9b631ea36ff6","2eef29965a6645318f8e701d3aa9f769","0bf176a7ffbc4f3c9037b15b6c4a1b88","689c9bc7efb9461ead60f70d8bb42d30","7ea99488f6cf4bd4be479f24e9118182","6b57bec9f9a946f993370c6dab8363ee"]},"id":"5jhFh7g_yxtg","executionInfo":{"status":"ok","timestamp":1618744358383,"user_tz":-180,"elapsed":37535,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"c321bc01-9952-451d-fbcb-0be7c3993509"},"source":["tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dd5ee21bce5462eb1b523155a5f201d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"129b2e31a1c54762a09eee2bc721a3a3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30ef4643b489481e9068699dd80bfb5a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iwWnT9-Tz4wX"},"source":["We're going to convert texts into tokens and append special [CLS] and [SEP] tokens as BERT expects. Also along with that, we want to calculate the length of each sentence after tokenization to see the distribution of lengths. That will help us to make a decision on truncation value as BERT can take maximum of 512 tokens at the input."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4LorgTWzQsB","executionInfo":{"status":"ok","timestamp":1618744470432,"user_tz":-180,"elapsed":149568,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"ac430d12-aadd-4413-b2ca-0f360a35e1de"},"source":["input_ids = []\n","lengths = []\n","for sentence in train_comments.comment:\n","  if (len(input_ids) + 1) % 20000 == 0:\n","    print(f\"Tokenized {len(input_ids)+1} comments.\")\n","  tokenized_comment = tokenizer.encode(sentence, add_special_tokens=True)\n","  input_ids.append(tokenized_comment)\n","  lengths.append(len(tokenized_comment))\n","print('DONE')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Tokenized 20000 comments.\n","Tokenized 40000 comments.\n","Tokenized 60000 comments.\n","DONE\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"CLSDn3oK1ikt","executionInfo":{"status":"ok","timestamp":1618744470434,"user_tz":-180,"elapsed":149556,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"2ddd749f-4f12-42c5-a5f8-1c61f73c380d"},"source":["# plotting the distribution of comment lengths\n","sns.set(style='darkgrid')\n","sns.set(font_scale=1.5)\n","plt.rcParams['figure.figsize'] = (10, 5)\n","lengths = [min(len, 512) for len in lengths]\n","sns.distplot(lengths, kde=False)\n","plt.title('Distribution of comments length')\n","plt.xlabel('Comment length')\n","plt.ylabel('Number of comments')\n","plt.show()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAoUAAAFjCAYAAABL3HHWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVhU5fs/8PcMmyK4YIAmoqQOoMhiWG65oiIuYAoKCZnmUlaKVkLq1afym6W4208T09RccgHBDfctRS1N0Y8ouSahMoiyOwxwfn/45Xwdh+WMzTAg79d1cV3O89znOffMA83dczaZIAgCiIiIiKhWkxs7ASIiIiIyPhaFRERERMSikIiIiIhYFBIRERERWBQSEREREVgUEhERERFYFBJVOzExMXB2dsaZM2eMuk9j5GHM/f4bmZmZ+Pzzz9GtWzc4OzsjNDTU2CmRDiIiIuDs7GzsNHQWGhqK3r17GzsNeomYGjsBopfVmTNnEBYWJr6Wy+WwsrKCvb092rVrh4EDB+Ktt96CTCbT2z6XLl0KV1dX+Pj46G1MQzhz5gzOnj2Ld999F/Xr1zd2Ov/a999/jz179mDixIlo3rw5XnnlFWOn9FL6+eefUb9+fbz99tvGTqXKxMTEIDs7G6NHjzZ2KlQLsCgkMrBBgwahe/fuEAQBeXl5uHXrFg4dOoQdO3agS5cuWLx4sUZh5O/vj4EDB8LMzEznfS1btgxDhw7VuSj8N/t8EWfPnhVzfb4orOpc9OHkyZPo1q0bPvroI2On8lJbt24dmjVrVquKwtjYWPzzzz8sCqlKsCgkMrC2bdvC399foy0yMhLz5s3DmjVrMHXqVKxatUrsMzExgYmJSZXklpubCysrqyrdZ2WqUy5SZWRkoGHDhsZOg4joX+E5hURGYGJigoiICLz++us4ceIE/vjjD7GvrHPqVCoVli5div79+8PDwwPe3t4YPHgwvv/+ewBAamqqeE5UbGwsnJ2dxZ9Szs7OiIiIQGJiIoKDg+Hl5YUPPvig3H2WKi4uxtKlS9GrVy+4ublh8ODB2L17t1Zc6fjPe37siIgILFu2DADQp08fMc+lS5dWmEtmZia++uor9OjRA25ubujRowe++uorPHr0qMz9JSYm4qeffoKPjw/c3NzQv39/xMbGljclWvLz8zF//nxx+65du+Lzzz/HP//8I8YsXboUzs7OEARB43OPiYmpcGxBELBlyxYEBgbCy8sLXl5eGDx4MBYvXvyv3/OyZcvQq1cvuLu7IzAwEBcuXADwdHU2ODgYnp6e6NatG3744QetvHr37o3Q0FBcvXoVo0ePhpeXFzp37ozvvvsORUVFUKlU+P777/HWW2+hffv2eOedd3Djxg2tcQoLC7FixQoMHDgQ7du3h7e3NyZOnIgrV65oxJ05c0b8vLZv346BAwfCzc0NvXr1QnR0tEass7Mz/vnnH5w9e1bj9zs1NRUAcP78ebz//vvo2rUr2rdvj7feegvjxo0T3/+LSE9Px5dffomePXvCzc0N3bp1w6xZs/Dw4UONuNLfg5s3b2LBggXo3r073NzcMGTIEBw7dkxr3IKCAsyZMwfdunWDu7s7goKCkJiYqHVuY+/evXH27Fn8888/Gu/5+b+NBw8eYOrUqejYsSM8PDwwduxY3Lp164XfN9VeXCkkMqLhw4fj3LlzOHbsGLy9vcuN++qrr7B9+3YEBATAy8sLxcXFuH37tvjlYGNjg7lz5+Lzzz+Ht7c3goKCyhzn8uXL2LdvH4KCgjB06FBJOUZFRSE/Px/BwcEAnhYgU6dOhUqleqHDeCNGjEBubi4OHDiAyMhINGrUCAAqPNE/JycHwcHBuHPnDoYNG4a2bdsiOTkZmzZtwunTp7F161ZYWVlpbLNw4UI8efIEI0aMgLm5OTZt2oSIiAg4Ojri9ddfrzBHtVqNsWPH4vz58+jfvz/ee+893LlzB5s2bcLJkyexfft2NGnSBH379oWjo6PW596hQ4cKx//ss8+wc+dOeHh4YOLEibC2tsbNmzexb98+TJ48+YXfc1RUFEpKShAWFga1Wo3Vq1djzJgxmDt3LmbMmIGgoCAMHjwYe/fuxZIlS+Dg4KC1in3//n2899578PPzQ//+/XHy5EmsWbMGJiYmuH79Op48eYLx48fj0aNHWL16NT788EPs3bsXcrlc47P7888/4e/vj3feeQe5ubnYsmULgoOD8csvv6B9+/Ya+9y8eTMyMjIwfPhw1K9fH/Hx8YiKikKTJk0wePBgAMDcuXMxZ84cNGrUCBMnThS3tbGxwc2bNzFmzBi88sorCAsLQ+PGjfHw4UOcO3cOV69ehaenZ4XzUZa0tDSMGDECarUaw4cPh6Ojo/g7cObMGWzfvh3W1tYa20RERMDU1BRjxoyBWq3G2rVrMWnSJCQkJMDBwUGMmzx5Mo4dOwYfHx906dIFqampmDRpkkYMAHzxxReYP38+Hj16hMjISLG9VatW4r/z8/MxatQoeHh4IDw8HKmpqVi3bh0+/PBD7Nq1q8atupORCURkEKdPnxYUCoWwatWqcmMuX74sKBQK4aOPPhLbtm/fLigUCuH06dNiW8eOHYX333+/0n0qFAph+vTp5fYpFArh5MmTWn1l7bO0rWfPnkJ2drbYnp2dLfTs2VPo2LGjUFBQUOm+yxp7yZIlgkKhEO7evSspfsGCBYJCoRB++eUXjdhffvlFUCgUwsKFC7W29/f3F1Qqldh+//59oV27dkJ4eHiZn8+zfv31V0GhUAjff/+9RvuRI0cEhUIhfPrppxrtFX3uz9u9e7c4RnFxsUbfs69f5D0HBARovOeDBw8KCoVCaNu2rZCUlCS2q1QqoWvXrkJQUJDG2L169RIUCoWwZ88ejfahQ4cKzs7OwsSJE4WSkhKxfe3atYJCoRCOHz8utq1Zs0arTRAEIScnR+jRo4cwatQosa30b6Rr164av2P5+fnCm2++WWZ+z27/fB4XL17U6pNi+vTpgkKh0GibOHGi0KlTJ+HevXsa7UlJSYKrq6uwZMkSsa3093n8+PEan8/FixcFhUIhREVFiW1Hjx4VFAqFMGPGDI1xS9ufz2PUqFFCr169ysx71KhRgkKhEFauXKnRHh0dXeYcEFWGh4+JjKh0pSc3N7fSuOvXryMlJeVf7c/FxQVdunTRaZvg4GCNFRFra2uMHDkSWVlZVXbbmAMHDsDGxgYjRozQaB8xYgRsbGxw8OBBrW1CQkJgbm4uvra3t4eTkxNu374taX9yuRwTJkzQaO/ZsydcXV1x6NAhlJSUvNB72blzJwBg+vTp4upaqWdfv8h7Dg4O1njPpavP7u7uGqtz5ubmaN++fZmfhb29PQYMGKDR1qFDBwiCgNDQUI2r5UvHv3PnjtgWHx+P1157De3atUNmZqb4U1hYiC5duuDcuXN48uSJxvjDhg3T+B2rW7cuPD09Jc0VAHHbQ4cOQaVSSdqmIjk5OTh69Ch69+4Nc3NzjffRrFkzODo64uTJk1rbhYWFaXw+7u7usLS01Ph8Dh8+DAB47733NLbt0aOHxgqgVHK5XOMuBwDQqVMnAJrzQiQFDx8TGVFpMfj8YcDnffHFF/j8888xePBgNG/eHG+++SZ69eqF3r17axUWFWnZsqXOOb722mtabaVfXqXncxlaamoq3NzcYGqq+Z8sU1NTtGzZUutcNQBo3ry5VlvDhg01zgmsaH92dnZo0KCBVl/r1q2RnJyMR48eoXHjxjq8i6fu3LkDW1vbSm9bo4/3XJr/84clS/seP36s1V5ebFl9pVeOPzvOjRs38OTJE3Tu3LnM9wUAjx49QtOmTSvcZ8OGDcvMrywDBw5EfHw8VqxYgZ9//hkeHh7o1q0bBg4ciGbNmkka41m3bt1CSUkJtm3bhm3btpUZU9bvV1ltjRo10jgHNDU1FXK5HI6OjlqxTk5OZZ6jWRE7OztYWFhotJVe9CT18yMqxaKQyIiuXbsG4OmXQUV8fHxw+PBhHDt2DL///jtOnTqFbdu2wdvbG2vWrNFYHapI3bp1/3XOuiouLq7yfQLQqVh+WZT3nnU5r6yi2PLGFwRB498KhULjHLjn2djYvHB+ZTE3N8eaNWuQlJQkXri1ZMkSLFu2DPPnz0ffvn11Gq/0/QwZMqTcc2+fL8QA3X7n9HV/0oo+u2fnhUgKFoVERlS6CtGjR49KYxs2bAh/f3/4+/tDEARERUVh1apVOHTokNbhPn26efOmVlvpasazKzzlrezcvXtXq03XL8TmzZvj1q1bKCoq0lg5Kyoqwu3bt8tcofk3mjdvjhMnTiA7O1vrPoo3btyAlZWVeIGMrlq2bIlDhw4hIyOjwtXCqn7P+tKiRQs8evQInTp1qvLC3N3dHe7u7gCAe/fuISAgAIsWLdK5KHR0dIRMJoNardb5dIvKNGvWDCUlJbhz547W4WJeMUzGVvv+V5qoGiguLsb333+Pc+fOoUePHhVeDVtcXIzs7GyNNplMhrZt2wIAsrKyxHZLS0u9HzLatGkTcnJyxNc5OTnYvHkz6tevjzfeeENsb9myJS5cuICCggKxLSsrq8zbs1haWmrlXhEfHx9kZmZi69atGu1btmxBZmam3p/g4uPjg5KSEqxcuVKj/dixY7hy5YrOh+2fVXo17bx587TOS3x2Zaeq37O+BAQEQKlUYs2aNWX2Z2RkvPDY9erVK/P3OzMzU6utSZMmsLGxkfw79qxGjRqhR48eOHDgQJm3tBEEocx9SlH6WLqff/5Zo/3YsWNlHjquV68esrKyuOpHVYIrhUQGduXKFcTFxQGAxhNN/vnnH3Tr1g3z58+vcPu8vDx069YNvXv3Rtu2bWFjY4PU1FRs2rQJDRo0QK9evcRYT09PJCYmYuXKlXj11Vchk8kwcODAf5V/o0aNEBgYKN5+JiYmBmlpaZg9e7bG4eh33nkHn332Gd599134+/sjOzsbW7duxauvvgqlUqkxpoeHB4Cnt1AZPHgwLCws0KZNGygUijJzeP/995GQkICvv/4aV65cgaurK5KTk7Ft2zY4OTnh/fff/1fv8XlDhw5FbGwsoqOj8c8//8Db2xt///03Nm7ciFdeeQVTp0594bEHDBiA/fv3Y8eOHbhz5w569+6N+vXr4/bt2/jtt9+wa9cuAFX/nvUlLCwMp06dwty5c3H69Gl06tQJVlZWSEtLw+nTp2Fubo7169e/0NgeHh7Ytm0bFi1ahFatWkEul6NXr15Yvnw5Tp48iZ49e8LBwQGCIODIkSO4efPmC39O//nPfxASEoJRo0bB398fbdu2RUlJCe7evYtDhw4hICAAH3/8sc7j9ujRA926dcOWLVvw6NEjdO7cGampqdiyZQucnZ3FU0qefc9HjhzB119/DS8vL5iYmKBTp04vdD4rUWVYFBIZ2K5du7Br1y7I5XJYWlqiSZMm6NixI/7zn/+ge/fulW5fp04dvPvuu0hMTERiYiLy8vJgZ2eH3r17Y8KECbC3txdjv/zyS3z99ddYsWIF8vLyAOBfF4Wffvop/vjjD2zcuBEZGRlwcnISi7lnDRkyBOnp6diwYQPmzJmD5s2b48MPP4RcLsfFixc1Yl9//XV8+umn2Lx5M2bNmoWioiJ89NFH5RaF1tbW2LRpE5YsWYLDhw8jJiYGjRs3xsiRI/Hxxx9XeqGOrszMzPDTTz9h+fLl2LNnDw4cOABra2v4+vpiypQpGhdJvIj58+fD29sb27Ztww8//AC5XA4HBwf4+vqKMVX9nvXFzMwMP/74IzZu3Ii4uDjxpuR2dnZo37695PtjliU8PBxZWVnYuHEjsrOzIQgCDh06BB8fHyiVSiQkJCAjIwN16tRBixYtMHv2bAwfPvyF9tW0aVNs374d0dHROHz4MOLj42FhYYGmTZuiV69eL3zKhkwmw9KlS7Fw4ULs3r0bx48fh7OzM5YtW4ZNmzZpXTE8evRo3L17F/v27cPmzZtRUlKCdevWsSgkg5AJXJMmIiIyusGDB0OtViMhIcHYqVAtxXMKiYiIqtDz92kEgKNHjyIlJQVdu3Y1QkZET/HwMRERURX64YcfcOXKFbz55puwtrZGcnIyYmJi0LBhQ4wbN87Y6VEtxsPHREREVejYsWNYuXIlrl+/jtzcXDRo0ACdOnXC5MmT0aJFC2OnR7UYi0IiIiIi4jmFRERERMSikIiIiIjAC0305tGjPJSU6P9IfOPGVnj4MFfv49KL4XxUH5yL6oNzUb1wPqqP6jgXcrkMjRrVK7OPRaGelJQIBikKS8em6oPzUX1wLqoPzkX1wvmoPmrSXPDwMRERERGxKCQiIiIiFoVEREREBBaFRERERAQWhUREREQEFoVEREREBBaFRERERAQWhUREREQEFoVEREREBD7R5KVTVAKo1EWVxlmYmcKU/0tARERE/4tF4UtGpS7C78kPKo3r6GoPUwtOPxERET3FtSIiIiIiYlFIRERERCwKiYiIiAgsComIiIgILAqJiIiICCwKiYiIiAgsComIiIgILAqJiIiICCwKiYiIiAgsComIiIgILAqJiIiICCwKiYiIiAgsComIiIgILAqJiIiICCwKiYiIiAgsComIiIgILAqJiIiICCwKiYiIiAgsComIiIgILAqJiIiICCwKiYiIiAgsComIiIgILAqJiIiICCwKiYiIiAgsComIiIgIeioKCwsL9TEMERERERmJ5KLw2LFjWLp0qUbbhg0b0KFDB3h6emLatGlQq9V6T5CIiIiIDE9yUfjTTz/h5s2b4usbN27g22+/hZ2dHbp06YI9e/Zgw4YNBkmSiIiIiAxLclF48+ZNuLm5ia/37NkDCwsLbNu2DatWrYKfnx927NhhkCRJ/2RyGfJURZJ+ikqMnS0REREZmqnUwKysLDRq1Eh8ferUKXTq1AlWVlYAgDfeeAPHjh3Tf4ZkECp1MS6mKCXFdnS1h6mF5F8VIiIiqoEkrxQ2atQIaWlpAIDc3FxcunQJ3t7eYn9RURGKi4v1nyERERERGZzk5R9PT09s3rwZrVu3xvHjx1FcXIzu3buL/Xfu3IGdnZ1BkiQiIiIiw5K8UvjJJ5+gpKQEU6ZMQUxMDAICAtC6dWsAgCAIOHjwIDp06KDTzm/fvo0pU6age/fu8PT0hJ+fH1auXKl1i5vz588jODgYHh4e6Nq1K2bPno2CggKt8QoLCzFv3jx069YN7u7uCAoKQmJiYpn7ljomERERUW0geaWwdevW2LNnD86fPw9ra2t07NhR7MvOzsa7776LN998U/KOHzx4gMDAQFhbW2PUqFFo0KAB/vjjD8yfPx9//fUX5s2bBwBITk7G6NGj0bp1a0REROD+/ftYvXo1UlNTsWLFCo0xIyIisH//foSFhaFFixaIjY3FuHHjsH79enh5eYlxuoxJREREVBtILgp///13tGrVCr1799bqa9CgAQYPHowbN25I3nFcXByys7OxceNGtGnTBgAwYsQIqFQq7NmzB99++y3MzMywYMECNGzYEOvXr0e9evUAAA4ODpg5cyYSExPRuXNnAEBSUhJ2796NyMhIjB49GgAQEBCAQYMGISoqSuN2OVLHJCIiIqotJB8+DgsLw8mTJ8vtP336NMLCwiTvOC8vDwDQuHFjjfZXXnkFpqamMDExQW5uLk6dOoWAgACxeAMAf39/WFpaYu/evWJbQkICzMzMEBgYKLZZWFhg+PDhOHfuHNLT0wFApzGJiIiIagvJRaEgCBX2FxcXQy6X/tS80sPPM2bMwNWrV3Hv3j3Ex8eLh3zlcjmuXbuGoqIijfsjAoC5uTlcXV2RnJwstiUnJ8PJyUmj0AMAd3d3CIIgxuoyJhEREVFtodPN52QyWbl9f/75p8Z9DCvTrVs3TJ48GT/++CMOHz4stn/yySeYNGkSAECpfHofPVtbW63tbW1tceHCBfG1UqmEvb19mXEAxJVCXcYkIiIiqi0qLArXrl2LdevWia+//fZbLFy4UCsuOzsbubm5GDZsmE47d3BwwBtvvIG+ffuiYcOGOHr0KJYuXQobGxsEBwfjyZMnAJ6u4j3PwsJC7AeAJ0+ewMzMrMw4AFCpVGKc1DF10bix1QttJ4WtrbXkWCEzH9ZWdSqNMzMzlRQHAJaWFrC1sZScw8tOl/kgw+JcVB+ci+qF81F91KS5qLAorF+/Pl599VUAwD///IOGDRtqnQMok8nQpk0beHp6ihd4SLF79258+eWXSEhIEFf4+vXrB0EQMHfuXPj5+aFOnadFy/O3qAGeFnml/QBQp04dqNXqMuOA/ysOdRlTFw8f5qKkpOJD7C/C1tYaSmWO5Ph8VRFycisvbNVqaXEAkJ+vgpI3Jgeg+3yQ4XAuqg/ORfXC+ag+quNcyOWycheyKiwKhw4diqFDhwIAevfujWnTpqFPnz56SWrjxo1o166d1iHf3r17IyYmBlevXhUP8ZYe8n2WUqnUuFm2ra2teIj4+TgAYqwuYxIRERHVFpKvDDl8+LDeCkIAyMjIKPOxeKWrfcXFxVAoFDA1NcXly5c1YgoLC5GcnAxXV1exzcXFBbdu3RKvai518eJFsR+ATmMSERER1RbSLxd+RkFBAe7du4e0tDStH6mcnJxw+fJl/P333xrtu3fvhomJCZydnWFtbY3OnTsjLi5Oo9iLi4tDfn4+fH19xTZfX1+o1Wps3bpVbCssLERMTAw6dOggrkjqMiYRERFRbSH56uOSkhKsWrUK69evR0ZGRrlxUm/pMnbsWBw/fhzBwcF455130KBBAxw9ehTHjx/HyJEjxXMXw8PDMXLkSISGhiIwMBD379/HmjVr0L17d3Tp0kUcz8PDA76+voiKioJSqYSjoyNiY2ORlpaGOXPmaOxb6phEREREtYVMqOwGhP9r7ty5WL16Ndq0aYM333wTDRs2LDPuo48+krzzpKQkLF26FMnJyXj8+DGaNWuGYcOGYezYsTAxMRHj/vjjD0RFReHKlSuwsrKCn58fpk6dCktLzStiVSoVFi1ahJ07dyIrKwvOzs6YOnVqmYWe1DGlqi4XmuSpivB78oNK4zwUtriYon1eZVk6utqjnoVOdy96aVXHk4ZrK85F9cG5qF44H9VHdZyLii40kVwUduvWDa6uroiOjtZrci8LFoW1Q3X8A6+tOBfVB+eieuF8VB/VcS4qKgoln1OYnZ2t1wtNiIiIiKj6kFwUKhSKMm/jQkREREQ1n+Si8KOPPsLmzZtx7949Q+ZDREREREYg+USxy5cv49VXX4Wfnx/69u0LBwcHyOWaNaVMJhOfW0xERERENYfkonDZsmXiv+Pj48uMYVFIREREVDNJLgoPHTpkyDyIiIiIyIgkF4XNmjUzZB5EREREZEQv9Ji7O3fu4Ny5c8jJqV733iEiIiKiF6NTUXjkyBH4+PjA19cXo0aNwuXLlwEADx8+RN++fZGQkGCQJImIiIjIsCQXhWfOnMFHH32EBg0aYNKkSXj2QSiNGzeGo6Mj9uzZY5AkiYiIiMiwJBeFP/zwA5ydnbF161a88847Wv2enp7473//q9fkiIiIiKhqSC4KL126hCFDhmjdm7BUkyZNkJGRobfEiIiIiKjqSC4KBUGAmZlZuf2PHj2qsJ+IiIiIqi/JReFrr72Gc+fOldt/5MgRuLi46CUpIiIiIqpakovC4cOHY9++fdi6dat4kYlMJkNBQQFmz56NCxcuICgoyGCJEhEREZHhSL55dUhICM6fP49Zs2bh+++/h0wmw7Rp0/D48WMUFxfj7bffxpAhQwyZKxEREREZiOSiEACioqLQv39/xMfH4+bNmxAEAe7u7ggICED//v0NlSMRERERGZhORSEA9O3bF3379jVELkRERERkJC/0mDsiIiIiernotFKYn5+PXbt24fbt23j8+LHGU02ApxeefPvtt3pNkIxPJpchT1VUaZyFmSlM+b8ZRERENZLkovD8+fP44IMPkJWVVW4Mi8KXk0pdjIspykrjOrraw9RC5zMSiIiIqBqQ/A0+e/ZsyOVy/L//9//g7e2N+vXrGzIvIiIiIqpCkovC69ev45NPPkHv3r0NmQ8RERERGYHkM8BsbW1haspDg0REREQvI8lFYWBgIHbt2oXi4mJD5kNERERERiB56W/ChAlIT0/HiBEjEBwcjGbNmsHExEQrrmPHjnpNkIiIiIgMT3JR+OTJEzx+/Bj//e9/MXPmTK1+QRAgk8mQnJys1wSJiIiIyPAkF4Vff/019u7dCx8fH7z++uto0KCBIfMiIiIioiokuSg8dOgQhg0bhtmzZxsyHyIiIiIyAskXmgiCgPbt2xsyFyIiIiIyEslF4RtvvIGLFy8aMhciIiIiMhLJReEXX3yBs2fPYs2aNSgsLDRkTkRERERUxSSfUxgWFoaCggLMnTsX8+fPh62tLeRyzZpSJpPh4MGDek+SiIiIiAxLclH46quvGjIPIiIiIjIiyUXh+vXrDZkHERERERmR5HMKiYiIiOjlJXml8FkFBQV4/PgxBEHQ6uNhZiIiIqKaR3JRWFxcjOjoaGzYsAEZGRnlxvExd0REREQ1j+SicM6cOfjll1/Qtm1b+Pr68jF3RERERC8RyUXhzp070a9fPyxZssSQ+RARERGREUi+0KSoqAhdu3Y1ZC5EREREZCSSi0IvLy9cv35d7wkkJSVh/Pjx6NixI7y8vDBkyBDExMRoxBw6dAhDhw5F+/bt0bNnTyxbtgxFRUVaY2VnZ2PWrFno1KkTPD09ERYWVu45jlLHJCIiIqoNJBeFn332GXbt2qXXJ5YcO3YMISEhKCoqwuTJkzF9+nR06dIF9+7d04iZNGkSGjRogFmzZsHHxwc//PAD5syZozFWSUkJxo8fj927d2PUqFH47LPP8PDhQ4SGhuLvv//W2q+UMYmIiIhqC8nnFDo7O+Obb77BJ598Ajs7Ozg4OJT5mLu1a9dKGi8nJweRkZEYOXIkZs6cWW7c3Llz0bZtW/z0008wMTEBANSrVw8rV65EaGgoWrZsCQBISEjAn3/+iR9++AE+Pj4AgAEDBqB///5YtmwZ5s6dq/OYRERERLWF5JXCo0ePYsqUKSgpKZEr4QcAACAASURBVEFubi7S0tKQmpqq8XP37l3JO965cyeys7MxefJkAEBubq7WfQ+vX7+O69evY8SIEWLxBgAhISEoKSnB/v37xbZ9+/bBzs4Offr0EdtsbGwwYMAAHDx4EGq1WucxiYiIiGoLySuF8+fPR9OmTbFs2TI4Ozv/6x0nJibitddew7FjxzBv3jzcv38f9evXx4gRIxAeHg4TExNcuXIFAODm5qaxrb29PZo0aSL2A0/vj9iuXTvIZDKN2Pbt2+PXX3/F33//jVatWuk0JhEREVFtIXml8M6dOwgNDdVLQVg63v379xEREYGhQ4di6dKl8PHxQXR0NL777jsAgFKpBADY2tpqbW9ra4v09HTxtVKphJ2dnVZcaVtprC5jEhEREdUWklcKX331VahUKr3tOD8/H1lZWZg2bRrGjx8PAOjXrx/y8/OxadMmfPDBB3jy5AkAwNzcXGt7CwsLFBQUiK+fPHlSZlxpW+lYuoypi8aNrV5oOylsba0lxwqZ+bC2qlNpnJmZqaQ4XWItLS1ga2MpacyaTJf5IMPiXFQfnIvqhfNRfdSkuZBcFIaGhmLdunUICQlBvXr1/vWO69R5WmQMGjRIo33w4MFISEjApUuXxJjCwkKt7VUqldhfOl5ZcaVtpbG6jKmLhw9zUVKi/Szof8vW1hpKZY7k+HxVEXJyn1Qap1ZLi9MlNj9fBWVxsaQxaypd54MMh3NRfXAuqhfOR/VRHedCLpeVu5AluSisV68erK2t4efnh7fffhsODg4aF2qUCggIkDSera0t/vrrL7zyyisa7aWvs7KyxEO8ZR0aViqV8PLy0hivrEO/pW2l2+syJhEREVFtIbkojIiIEP+9fPnyMmNkMpnkorBdu3Y4deoUHjx4gObNm4vt9+/fB/D0ymF7e3sAwOXLl9GuXTsx5sGDB7h//z5cXV3FNhcXF/z5558QBEHjYpOkpCRYWlrC0dERAMRtpIxJREREVFtILgrXrVun1x37+voiOjoa27ZtQ3h4OABAEARs3boVlpaW8PT0hJWVFV577TX8+uuvGD58uLgyuWnTJsjlcvTr109jvH379uHQoUPifQozMzORkJCAPn36wMzMDADQpk0byWMSERER1RaSi8I33nhDrzt2c3NDQEAAfvzxRzx8+BBt27bFsWPH8Ntvv+Gzzz6DldXT492ff/45PvjgA4wdOxZ+fn5ISUnBhg0bMGLECDg5OYnj9e/fH56envj8888xZswYNGrUCJs2bUJJSQk+/vhjjX1LHZOIiIiotjD5z3/+858X2TAzMxMFBQWoW7fuC++8R48eEAQB+/fvx759+yAIAsLDwxEWFibGODk5wcXFBcePH8eOHTtw7949hIWFYdq0aRpPVCld5UtPT8eOHTtw/PhxtGjRAvPnz0ebNm009it1TF0UFBRC0P91JqhXzwL5+doXxZRHXVyCtIy8SuOaNK6HBw/zJY0pNbaZrRXMTV/s86spdJ0PMhzORfXBuaheOB/VR3WcC5lMBktL7TuwAIBMeP4xIhV48OABFixYgEOHDiEv72nhYWVlhT59+iA8PFw8B7A2qi5XH+epivB78oNK4zwUtriYopQ0ptTYN9o1gSDhM7AwM0VNrR2r45VktRXnovrgXFQvnI/qozrOhV6uPk5LS0NQUBAyMjLg6uqK1q1bAwBu3LiBHTt24OTJk9iyZQuaNm2qn6ypxlGpiyUVjx1d7WFqIflXj4iIiKqA5G/mxYsXIzs7Gz/++CN69Oih0Xfs2DF8/PHHWLx4sfg0EiIiIiKqOSQfxDt58iRCQkK0CkLg6bmBwcHBOHHihF6TIyIiIqKqIbkozMrKQosWLcrtb9GiBbKzs/WSFBERERFVLclFYZMmTXD27Nly+//44w80adJEL0kRERERUdWSXBT6+voiISEB8+fPR07O/11Jk5ubiwULFmDv3r3w8/MzSJJEREREZFiSLzT58MMP8ccffyA6OhqrV68Wnxucnp6O4uJidOjQAR988IHBEiUiIiIiw5FcFNatWxfr169HTEwMDh48iNTUVABAt27d4OPjg6FDh8LUlLcZISIiIqqJdKriTE1NERQUhKCgIEPlQ0RERERGIPmcwqKiIuTm5pbbn5ubi6KiIr0kRURERERVS3JR+N1332HYsGHl9g8bNgxRUVF6SYqIiIiIqpbkovC3335Dv379yu3v378/jh8/rpekiIiIiKhqSS4K79+/D0dHx3L7mzdvjnv37uklKSIiIiKqWpKLQjMzM6Snp5fbr1QqIZdLHo6IiIiIqhHJVZyLiwsSEhJQWFio1adWq7F37144OzvrNTkiIiIiqhqSi8JRo0bhr7/+woQJE3Dp0iUUFhZCrVbj0qVLmDBhAq5fv45Ro0YZMlciIiIiMhDJ9yns378/JkyYgB9//BFBQUGQyWSQyWQoKSmBIAgYN24cH3NHksjkMuSpKr99kYWZKUx5RgIREVGV0Onm1eHh4ejTpw/i4+Px999/AwBatmyJQYMGwd3d3SAJ0stHpS7GxRRlpXEdXe1hasGn5BAREVUFnb9x3d3dWQASERERvWR4cI6IiIiIWBQSEREREYtCIiIiIgKLQiIiIiICi0IiIiIiQgVFYZ8+fXDo0CHx9bJly5CSklIlSRERERFR1Sq3KLx37x7y8vLE18uWLcO1a9eqJCkiIiIiqlrlFoX29vZaK4MymczgCRERERFR1Sv35tV9+vTBqlWrcOLECTRo0AAAsHz5cmzZsqXcwWQyGdauXav/LImIiIjIoMotCj/99FPUr18fp06dQlpaGmQyGTIzM1FQUFCV+RERERFRFSi3KKxTpw4++eQTfPLJJwAAFxcXfPHFFxg8eHCVJUf/p6gEUKmLKo0rEaogGSIiInrpSH728Zw5c+Dl5WXIXKgCKnURfk9+UGmch8K2CrIhIiKil43konDo0KHivx89eoTU1FQAgIODAxo1aqT/zIiIiIioykguCgHg6tWrmD17Ns6dO6fR7u3tjRkzZsDFxUWvyRERERFR1ZBcFKakpCA4OBiFhYXo06cPWrduDQC4fv06jhw5gnfeeQebN29GmzZtDJYsERERERmG5KJwyZIlMDMzw6ZNm7RWBFNSUjBq1CgsWbIES5cu1XuSRERERGRYkp99/PvvvyMkJKTMQ8QKhQLBwcE4e/asXpMjIiIioqohuSgsKCiArW35V7ba2dnxHoZERERENZTkorB58+Y4cuRIuf1HjhxB8+bN9ZIUEREREVUtyUWhv78/fvvtN0ybNg1//fUXiouLUVxcjJSUFEybNg0nT57UuG0NEREREdUcki80GTt2LK5cuYLdu3djz549kMuf1pMlJSUQBAEDBgzAmDFjDJYoERERERmO5KLQxMQEixYtwsmTJ3Hw4EHx5tXNmzeHj48PunTpYrAkiYiIiMiwdLp5NQB07doVXbt2NUQuiI6ORlRUFFxcXBAXF6fRd/78ecybNw9XrlyBlZUVBgwYgGnTpqFu3boacYWFhVi8eDHi4uKQnZ0NFxcXhIeHo3Pnzlr7kzomERER0ctO8jmFhqZUKrF8+XJYWlpq9SUnJ2P06NFQqVSIiIjA8OHD8euvvyI8PFwrNiIiAmvXrsWQIUMwY8YMyOVyjBs3Dn/++ecLj0lERET0stN5pdBQ5s+fDzc3NwiCgOzsbI2+BQsWoGHDhli/fj3q1asH4Okzl2fOnInExERxFTApKQm7d+9GZGQkRo8eDQAICAjAoEGDEBUVhQ0bNug8JhEREVFtUC1WCpOSkhAfH4/IyEitvtzcXJw6dQoBAQFi8QY8vRra0tISe/fuFdsSEhJgZmaGwMBAsc3CwgLDhw/HuXPnkJ6ervOYRERERLWB0YtCQRDwzTffICAgAK6urlr9165dQ1FREdzc3DTazc3N4erqiuTkZLEtOTkZTk5OGoUeALi7u0MQBDFWlzGJiIiIagOjF4U7duzA9evXMWXKlDL7lUolAJT5NBVbW1tx9a801s7Orsw4AGKsLmMSERER1QaSzil88uQJEhIS4OTkBA8PD73tPDc3F/Pnz8f48ePLLOZK9w08XcV7noWFhdhfGmtmZlZmHACoVCqdx5SqcWMrnbeRytbWGkJmPqyt6lQaa2Zmqtc4Q4wpNc7S0gK2NtoXHhmbra21sVOg/8W5qD44F9UL56P6qElzIakoNDc3x8yZMzFjxgy9FoXLly+HmZkZ3nvvvXJj6tR5WjwUFhZq9alUKrG/NFatVpcZB/xfcajLmFI9fJiLkhJB5+0qY2trDaUyB/mqIuTkVl6sqtX6jTPEmFLj8vNVUBYXS8qxqpTOBxkf56L64FxUL5yP6qM6zoVcLit3IUtSUSiXy9G0aVPk5ubqLan09HSsXbsWkydPRkZGhtiuUqmgVquRmpoKa2tr8RBv6SHfZz1/uLi8Q7+l25bG6jImERERUW0g+ZzCgIAAxMfHl7m69iIePnwItVqNqKgo9OnTR/y5ePEibty4gT59+iA6OhoKhQKmpqa4fPmyxvaFhYVITk7WuDjFxcUFt27dQl5enkbsxYsXxX4AOo1JREREVBtIvk9hhw4dcODAAfj7+yMkJAQtWrQo88kfHTt2lDSeg4MDfvjhB632RYsWIT8/H1988QVatmwJa2trdO7cGXFxcZgwYYJ4ZXFcXBzy8/Ph6+srbuvr64vVq1dj69at4n0KCwsLERMTgw4dOsDe3h4AdBqTiIiIqDaQXBQ+e97f//zP/0Amk2n0C4IAmUwm+XYu1tbW8PHx0Wpfu3YtTExMNPrCw8MxcuRIhIaGIjAwEPfv38eaNWvQvXt3jWcue3h4wNfXF1FRUVAqlXB0dERsbCzS0tIwZ84cjf1IHZOMRyaXIU9VVGmchZkpTI1+HT0REVHNJrkofL6oqkrt2rXDmjVrEBUVhTlz5sDKygpBQUGYOnWqVuzcuXOxaNEixMXFISsrC87Ozli5ciVef/31Fx6TjEOlLsbFFO3zPp/X0dUephbV5uE8RERENZLkb9KhQ4caMg/R+vXry2z39vbG5s2bK93ewsIC06dPx/Tp0yuNlTomERER0cuOB92IiIiISLei8N69e4iMjET37t3h5uaGxMREAEBmZiYiIyORlJRkkCSJiIiIyLAkF4V3797FsGHDsH//frRp0wbFz9xU2MbGBpcvX8a2bdsMkiQRERERGZbkcwoXLVoEuVyOXbt2wcLCQusK3R49euDIkSN6T5CIiIiIDE/ySuGpU6cQHByMpk2bat2OBgBeffVV3L9/X6/JEREREVHVkFwU5ubmVvj4N7VarXFImYiIiIhqDslFYdOmTfHXX3+V23/x4kU4OjrqJSkiIiIiqlqSi8K+ffti+/btSElJEdtKDyPv27cPCQkJGDBggP4zJCIiIiKDk3yhyQcffICjR48iKCgI3t7ekMlkiI6OxsKFC5GUlARXV1eMGTPGkLkSERERkYFIXim0srLCr7/+iuHDh+Py5csQBAEnT57ErVu3EBISgnXr1sHCwsKQuRIRERGRgej0wFgrKyvMnDkTM2fORGZmJgRBgI2NTZlXIxMRERFRzaFTUfgsGxsbfeZBREREREakc1G4Z88eHDx4EHfv3gUANG/eHD4+PvDz89N7ckRERERUNSQXhfn5+Zg0aRJOnz4NQRBQv359AMClS5ewd+9e/Prrr1i+fDksLS0NliwRERERGYbkC00WLlyIxMREjBo1CidOnMDZs2dx9uxZnDhxAqNGjcKZM2ewcOFCQ+ZKRERERAYiuSjcu3cvfH19MWPGDNja2orttra2mDFjBvr164e9e/caJEkiIiIiMiydHnP35ptvltvfqVMn5Obm6iUpIiIiIqpakotCZ2dn3Llzp9z+O3fuQKFQ6CUpIiIiIqpakovCKVOmYMuWLTh8+LBW38GDB7F161aEh4frNTkiKWRyGfJURZX+FJUYO1MiIqLqq9yrjyMjI7XaHBwcMGnSJDg5OaFVq1YAgBs3buDWrVtQKBTYuXMnOnfubLhsicqgUhfjYoqy0riOrvYwtXjhW3MSERG91Mr9hoyNjS13o5s3b+LmzZsabdeuXUNKSgq+/fZb/WVHRERERFWi3KLw6tWrVZkHERERERmR5HMKiYiIiOjlxaKQiIiIiHR79vH58+exYcMG3LlzB48fP4YgCBr9MpkMBw8e1GuCRERERGR4kovCLVu24Msvv4SZmRmcnJzQtGlTQ+ZFRERERFVIclG4YsUKuLq6YtWqVbCxsTFkTkRERERUxSSfU/jw4UMMGzaMBSERERHRS0jySmGrVq2QnZ1tyFyIDKr0ySeVsTAzhSkvwSIiolpGclE4ceJEfPPNN3j77bdhb29vyJyIDIJPPiEiIiqf5G++fv36oaCgAAMHDkSfPn3QrFkzyOWayykymQyTJk3Se5JEREREZFiSi8Jbt25hyZIlyM3NRVxcXJkxLAqJiIiIaibJReFXX32FzMxMzJgxA97e3qhfv74h8yIiIiKiKiS5KLxw4QLGjh2L0NBQQ+ZDREREREYg+RpLKysr3o6GiIiI6CUluSgcMGAA9u/fb8hciIiIiMhIJBeFI0eORF5eHj788EMkJibi7t27SEtL0/ohIiIioppH8jmFAwcOhEwmw+XLl3HkyJFy45KTk/WSGBERERFVHclF4aRJkyCTyQyZCxEREREZieSi8OOPPzZkHkRERERkRHzCKxERERFJXyn8/fffJcV17NhRUlxSUhJiY2Nx5swZpKWloWHDhvDy8sKUKVPQokULjdjz589j3rx5uHLlCqysrDBgwABMmzYNdevW1YgrLCzE4sWLERcXh+zsbLi4uCA8PBydO3fW2r/UMYmIiIhqA8lFYWhoqKRzCqVeaLJq1SqcP38evr6+cHZ2hlKpxIYNGxAQEIBt27ahVatW4nijR49G69atERERgfv372P16tVITU3FihUrNMaMiIjA/v37ERYWhhYtWiA2Nhbjxo3D+vXr4eXlpZGj1DGJiIiIagPJReGcOXO02oqKinD37l3ExMTAwcEBI0aMkLzj0aNHIyoqCubm5mKbn58fBg8ejOjoaHz33XcAgAULFqBhw4ZYv3496tWrBwBwcHDAzJkzkZiYKK4CJiUlYffu3YiMjMTo0aMBAAEBARg0aBCioqKwYcMGcT9Sx6TaSSaXIU9VVGafkJmP/Gf6LMxMYcqTMIiI6CUguSgcOnRouX1jx46tsL8sHTp00Gpr2bIl2rRpgxs3bgAAcnNzcerUKYwdO1Ys3gDA398f3377Lfbu3SsWcAkJCTAzM0NgYKAYZ2FhgeHDh2PhwoVIT0+HnZ2dTmNS7aRSF+NiirLMPmurOsjJfSK+7uhqD1MLyX9GRERE1ZZe1jgaNGiAwMBArFq16l+NIwgCMjIy0KhRIwDAtWvXUFRUBDc3N404c3NzuLq6ahyqTk5OhpOTk0ahBwDu7u4QBEGM1WVMIiIiotpCb0sc9evXx927d//VGPHx8Xjw4AHCw8MBAErl09UaW1tbrVhbW1tcuHBBfK1UKmFvb19mHACkp6frPKYuGje2eqHtpLC1tYaQmQ9rqzqVxpqZmeo1zhBj1vS4Z/ssLS1ga2NZ6ZhkGLa21sZOgf4X56J64XxUHzVpLvRSFKpUKsTHx+OVV1554TFu3LiBr7/+Gq+//jr8/f0BAE+ePD1M9+x5h6UsLCzE/tJYMzOzMuNKc9R1TF08fJiLkhLhhbatiK2tNZTKHOSrijQOW5ZHrdZvnCHGrMlxzx8+LnhSiNupqkrH5LmH+lf6t0HGx7moXjgf1Ud1nAu5XFbuQpbkojAyMrLM9qysLFy4cAGZmZn4/PPPXyhBpVKJCRMmoEGDBli8eDHk8qffnnXqPF2RKSws1NpGpVKJ/aWxarW6zDjg/4pDXcYkqkxF5x8+i+ceEhFRdSf5Wyo2NrbM9gYNGsDJyQmRkZEYPHiwzgnk5ORg3LhxyMnJwaZNmzQO65b+u/SQ77OUSiXs7Ow0YksPET8fB0CM1WVMIiIiotpCclF49epVve9cpVJh4sSJuH37Nn7++We89tprGv0KhQKmpqa4fPky+vXrJ7YXFhYiOTlZowh1cXHB+vXrkZeXp3GxycWLF8V+XcckIiIiqi2MdpZTcXExpkyZggsXLmDx4sXw9PTUirG2tkbnzp0RFxeHvLw8sT0uLg75+fnw9fUV23x9faFWq7F161axrbCwEDExMejQoYN4EYouYxIRERHVFkY7yem7777D4cOH0atXLzx+/BhxcXFiX7169eDj4wMACA8Px8iRIxEaGorAwEDcv38fa9asQffu3dGlSxdxGw8PD/j6+iIqKgpKpRKOjo6IjY1FWlqa1o23pY5JREREVFtUWBROnDhRp8FkMhmWL18uKbb0cPSRI0dw5MgRjb5mzZqJRWG7du2wZs0aREVFYc6cObCyskJQUBCmTp2qNebcuXOxaNEixMXFISsrC87Ozli5ciVef/11jThdxiQiIiKqDSosCo8eParTYFKejVxq/fr1kmO9vb2xefPmSuMsLCwwffp0TJ8+XW9jEhEREf0bRSWASl3241OfZezbl1VYFEq5uOTs2bOYN28eLl26VOYNoYmIiIhqM5W6CL8nP6g0zti3L3vhPaekpCAqKgonTpxAvXr1MHnyZLz33nv6zI2IiIiIqojOReG9e/ewePFi7Ny5E3K5HKGhofjggw/E5xUTERERUc0juSjMysrCihUrsHHjRhQWFmLgwIGYMmUKHBwcDJkfEREREVWBSovCwsJC/Pzzz1i1ahWys7PRtWtXfPrpp3B1da2K/IiIiIioClRYFG7duhXLli1Deno62rZti08//RSdO3euqtyIXhoyuQx5qup/5RkREdVeFRaFs2bNgkwmg5ubGwYMGICrV69WeEWyTCbD6NGj9Z0jUY2nUhfjYor287afZ+wrz4iIqPaq9NtHEARcunQJly5dqnQwFoVERERENVOFReG6deuqKg8iIiIiMqIKi8I33nijqvIgIiIiIiPiyUtE1QgvSCEiImNhUUhUjfCCFCIiMhauNRARERERi0IiIiIi4uFjohqJ5x4SEZG+sSgkqoF47iEREekb1xCIiIiIiCuFRC8zHmYmIiKpWBQSvcR4mJmIiKTitwARcUWRiIhYFBIRVxSJiIgXmhARERERWBQSEREREVgUEhERERFYFBIRERERWBQSEREREVgUEhERERF4Sxoi0oHU+xkCgJmpKdRFvPchEVFNwaKQiCSTej9DAPBQ2PLeh0RENQj//5yIiIiIWBQSEREREQ8fE5GR8bnLRETVA4tCIjIqqecpvtGuCVRqQXwtZOYjv4xikhe4EBG9GBaFRFQjPF88WlvVQU7uE604XuBCRPRi+F9EIqqVeNiaiEgTi0IiqpVe9LB1eVg8ElFNx6KQiKgC+i4eARaQRFQ9sSgkItIDXW7szdVHIqqOWBQSEVUxfa8+8oprItIHFoVERNWU1OJR6hXXXKEkooqwKCQiqiWkFpm8XQ9R7VSr/+oLCwuxePFixMXFITs7Gy4uLggPD0fnzp2NnRoRkdFIvV1P6WHr8m4k/nycFFylJDKeWl0URkREYP/+/QgLC0OLFi0QGxuLcePGYf369fDy8jJ2ekRERqHrYevybiT+fJwU+j6P0lhxLG6pJqq1RWFSUhJ2796NyMhIjB49GgAQEBCAQYMGISoqChs2bDBugkREtZC+z6M0Vpy+i1uAhSYZXq0tChMSEmBmZobAwECxzcLCAsOHD8fChQuRnp4OOzs7I2ZIREQ1lb6LW0B6oSl7nK/T4f/qGqdLEVxUAqjUXMH9t2ptUZicnAwnJyfUq1dPo93d3R2CICA5OVmnolAul+k7RY2xTU3ksKxjVmmsvuMMMWZNjqtrYYriIjNJscbKsTrEVcW+n5+Lqtrvv42rCTnqGlfeXOg6niFzrOlxAFBcIiD5Vmalce4KW/xXQpyrk42k8YwV56GwRXGRtBvClwiQ9J6ljmlqaoKiouJ/HZfxuACqohLI5dJ/HwxZTwAV1ysyQRCkfeIvmUGDBsHe3h4//fSTRvv169cxcOBAzJ49W2MVkYiIiOhlVmsXUZ88eQIzM+2q3cLCAgCgUqmqOiUiIiIio6m1RWGdOnWgVqu12kuLwdLikIiIiKg2qLVFoa2tLdLT07XalcqnJ/zyIhMiIiKqTWptUeji4oJbt24hLy9Po/3ixYtiPxEREVFtUWuLQl9fX6jVamzdulVsKywsRExMDDp06AB7e3sjZkdERERUtWrtLWk8PDzg6+uLqKgoKJVKODo6IjY2FmlpaZgzZ46x0yMiIiKqUrX2ljTA04tKFi1ahJ07dyIrKwvOzs6YOnUqunTpYuzUiIiIiKpUrS4KiYiIiOipWntOIRERERH9HxaFRERERMSisDoqLCzEvHnz0K1bN7i7uyMoKAiJiYnGTuulkp6ejqioKISGhsLLywvOzs44c+ZMmbGHDh3C0KFD0b59e/Ts2RPLli1DURkPc8/OzsasWbPQqVMneHp6IiwsDMnJyYZ+KzVeUlISvvrqK/j5+cHT0xM9e/ZEeHg47ty5oxV7/vx5BAcHw8PDA127dsXs2bNRUFCgFce/oRdz6dIlTJo0Cb169YK7uzu6du2KsWPH4vz581qxnIuqFx0dDWdnZ/j7+2v1cT4M68yZM3B2di7z58aNGxqxNXkueE5hNTR16lTs378fYWFhaNGiBWJjY3H58mWsX78eXl5exk7vpXDmzBnx87WxscGff/6JdevW4c0339SIO3bsGCZMmIBOnTrBz88PKSkp2LBhA0JCQjBr1iwxrqSkBCEhIUhJScGYMWPQqFEjbNy4EQ8ePEBMTAwcHR2r+i3WGJ988gnOnz8PX19fODs7Q6lUYsOGDcjPz8e2bdvQBQnvsgAAEqlJREFUqlUrAEBycjJGjBiB1q1bIzAwEPfv38fq1avRtWtXrFixQmNM/g29mD179iA+Ph7u7u6wtbVFTk4Odu7ciWvXriE6Ohpdu3YFwLkwBqVSif79+0MQBDg6OiIuLk7s43wYXul3xrvvvot27dpp9PXp0wdWVlYAXoK5EKhauXjxoqBQKIQ1a9aIbU+ePBF8fHyEkJAQ4yX2ksnJyREyMzMFQRCEAwcOCAqFQjh9+rRWnJ+fnzB06FChqKhIbFuwYIHg4uIi3Lp1S2zbvXu3oFAohAMHDohtDx8+FLy9vYXPPvvMcG/kJXDu3DlBpVJptN26dUtwc3MTpk+fLra9//77wltvvSXk5uaKbVu2bBEUCoVw6tQpsY1/Q/qVn58vdOnSRRg/frzYxrmoetOnTxdCQ0OFUaNGCUOGDNHo43wY3unTp7X+G1+Wmj4XPHxczSQkJMDMzAyBgYFim4WFBYYPH45z586V+Wg+0p2VlRUaNWpUYcz169dx/fp1jBgxAiYmJmJ7SEgISkpKsH//frFt3759sLOzQ58+fcQ2GxsbDBgwAAcPHizzOdv0VIcOHWBubq7R1rJlS7Rp00Y8LJObm4tTp04hICAA9erVE+P8/f1haWmJvXv3im38G9KvunXrwsbGBtnZ2QA4F8aQlJSE+Ph4REZGavVxPqpebm5umacQvQxzwaKwmklOToaTk5PGLxQAuLu7QxAEnqNWha5cuQIAcHNz02i3t7dHkyZNxH7g6by1a9cOMplMI7Z9+/bIy8vD33//bfiEXyKCICAjI0Ms3K9du4aioiKtuTA3N4erq6vG3wX/hv693NxcZGZm4ubNm1iwYAFSUlLQuXNnAJyLqiYIAr755hsEBATA1dVVq///t3evQU2cXRzA/wUDErkojliVQmEmCVbCJVS5qoOgglOqlEEQDUWsSEFbYdqK9gOVTuulaitCvVBhZFRKRRS1CjbUDqOtt3YQpdEKXgq1KBoJBJoYYd8PvNmXNeHlIgaq5zfDh5x99uHZPSxzsvvsLuXDuD788EN4eXnB3d0d8fHxuHbtGrvsecjFC/tGk6GqsbHR4Cv2xowZAwD0Tc6IGhsbAfxv33c1ZswYTi4aGxvh4+Oj187Ozg5AZ950c+NIz44cOYK7d+8iJSUFQM+5qKysZD/TMfT01qxZg7KyMgAAj8dDdHQ0EhMTAVAujO3w4cOoqalBdna2weWUD+Pg8XiYPXs2pk2bhlGjRuHatWvIzc1FTEwMioqK4OTk9FzkgorCIUatVoPH4+nFzc3NAXS+hYUYh1qtBgC9S5tAZz663k2mVqsNttPFdH2RntXW1iIjIwNeXl7sXZY95aLr/qVj6OklJycjKioKDQ0NKCkpwaNHj6DVamFmZka5MCKVSoXNmzcjISGB/YL5JMqHcUgkEkgkEvZzUFAQZsyYgYiICGRlZWHz5s3PRS7o8vEQM3z4cIPzz3R/ILo/GPLsDR8+HEDnYwOepNFo2OW6toba6WJd25LuNTY2YtmyZbCxscHWrVthYtL5L6qvuaBj6OmIRCL4+/sjIiICu3fvRnV1NTufjXJhPNu3bwePx8PixYu7bUP5GDwuLi7w9fXF2bNnATwfuaCicIh58rKkju60dHffFsnA053G1+37rhobGzm56C5vuhjlrWctLS1YunQpWlpa8M0333AuwQxELugY6h8ej4egoCCcPHkSarWacmEk9+7dw549exATE4P79++jvr4e9fX10Gg00Gq1qK+vh1KppHwMsnHjxkGpVAJ4Pv5PUVE4xLi4uODmzZtobW3lxC9dusQuJ8ahm9R95coVTvzu3btoaGjgTPp2cXFBdXU1mCce+1lVVQU+n0/PKeyBRqNBYmIibt26hZ07d8LZ2ZmzXCgUYtiwYXq5ePToEeRyuV4u6BgaWGq1GgzDoLW1lXJhJA8ePIBWq8WmTZsQFBTE/ly6dAm1tbUICgpCTk4O5WOQ1dXVsTfEPQ+5oKJwiAkJCYFWq8WBAwfY2KNHj1BcXAyJRGJwYip5NgQCAZydnVFYWIj29nY2XlBQABMTE8yaNYuNhYSE4N69eygvL2djCoUCpaWlCAoKMjh3hHRqb2/HypUrUVlZia1bt8LDw0OvjZWVFXx9fVFSUsL5J1pSUoK2tjaEhISwMTqG+k+hUOjFVCoVysrKMG7cOIwePZpyYST29vbIzs7W+xEIBJgwYQKys7Mxb948yoeRGDo2Ll68iHPnziEgIADA8/F/it5oMgS9//77KC8vx9tvvw0HBwf2Ked79uyBl5fXYA/vufH1118D6Lyx4dixY4iIiIC9vT2sra2xaNEiAMCpU6fw7rvv6r3RJCoqCp988gnbV3t7O2JiYnD9+nX2jSYFBQX4+++/UVxcDEdHx8HYxH+Fzz77DPn5+QgMDERoaChn2YgRIxAcHAwAqK6uRnR0NAQCAfumgLy8PHh7eyMnJ4ezHh1D/RMbGwtzc3N4enpizJgx7N9vQ0MDtmzZgjlz5gCgXAwmqVSK5uZmzhtNKB/PXmxsLCwsLODp6YlRo0bh+vXrKCwshJWVFYqKijB+/HgA//5cUFE4BGk0Gnz11Vc4evQolEolRCIRUlNT4efnN9hDe66IRCKD8QkTJuDHH39kP8tkMmRlZaG2tha2traIiIhAUlIShg3j3ryvVCqxceNGyGQyaDQaiMVipKWl6b0SiXBJpVKcP3/e4LInc3Hx4kVs2rQJv//+OywtLTFnzhykpqaCz+dz1qNjqH+KiopQUlKCmpoaNDc3w8rKCh4eHoiPj8eUKVM4bSkXg8NQUQhQPp61/Px8HD16FH/++SdUKhVsbW0REBCAFStWsAWhzr85F1QUEkIIIYQQmlNICCGEEEKoKCSEEEIIIaCikBBCCCGEgIpCQgghhBACKgoJIYQQQgioKCSEEEIIIaCikBBCCCGEgIpCQgh5IWzbtg0ikQj19fWDPZQ+SUtL6/ZB84SQgTWs5yaEEDLw/vnnHxQWFuLkyZOoqalBa2srbGxsMGnSJISGhuLNN9/Ue2sM6SSXyyGTyRAeHg57e/vBHs5Tk8lkkMvlWLFixWAPhZAXGp0pJIQY3e3btzFv3jysW7cO5ubmSEhIQEZGBuLi4vD48WOsXr0aW7ZsGexhDllyuRxZWVn466+/BnsoA0L3KklCyOCir+GEEKNSq9VYtmwZ6uvrsW3bNsyaNYuzPCEhAVVVVbh8+fIgjZAQQl5MdKaQEGJUBw4cwM2bN7F48WK9glDHzc0NCxcu5MRkMhmio6Ph4eEBT09PREdHQyaT6a07Y8YMSKVSXL16FXFxcfD09ISvry/Wr1+Px48fQ6PRYMOGDZg6dSrEYjEWLlyI2tpaTh/FxcUQiUT45ZdfkJWVhcDAQLi5uSEyMhKVlZUAgPPnz2PBggXw8PBAQEAAsrOzDW7L5cuXkZycDG9vb7i6umL27NnYvn07Hj9+zGknlUoxY8YM3L17F6mpqZg8eTLc3d2xZMkS3Lx5k223bds2rF69GgAQGxsLkUgEkUiEtLS0Hva8YS0tLfjiiy8wc+ZMuLq6wsfHB6mpqairq+t2n+zevRvBwcHs9hw6dEiv3/b2dmRnZyMwMBBisRhhYWE4fvy43txGqVTKrq/bFpFIhOLiYr1xpqenw9fXF2KxGNHR0bh06VK/tpkQYhidKSSEGFVZWRkAICoqqtfr7Nu3DxkZGXB2dkZSUhIA4NChQ0hOTkZGRoZeXw0NDVi8eDHmzJmD2bNn48yZM8jLy4OpqSlqamqgVquRkJCAhw8fIjc3F0lJSThx4gRMTLjfkzdt2oSOjg7ExsZCq9UiNzcX8fHx2LhxIz7++GPMnz8fYWFhOHHiBDIzM2Fvb4+5c+ey6//0009Yvnw5HB0dER8fDxsbG1RWViIzMxNyuRyZmZmc39fW1oZFixbB3d0dKSkpqK+vR35+PpKSknDs2DGYmppi5syZaGxsRGFhIRITE+Hs7AwAcHBw6H0S/qulpQXR0dG4c+cOIiIiIBAI0NjYiP379yMyMhIHDx7EhAkTOOt8+eWXUKvViIqKgpmZGQoKCpCWlgYHBwd4eXmx7TIyMvDtt9/C29sb8fHxUCgUWLt2rV5/iYmJ6OjowMWLF7Fx40Y2LpFIOO2WLFkCW1tbJCcno6mpCXl5eUhISEB5eTksLS37vO2EEAMYQggxoilTpjASiaTX7ZuamhgPDw8mODiYaWlpYeMtLS1MUFAQ4+HhwSiVSjYeGBjICIVC5vjx45x+wsPDGZFIxCQmJjIdHR1sfM+ePYxQKGQqKirY2MGDBxmhUMjMmzeP0Wg0bFwmkzFCoZB57bXXmKqqKjau0WgYf39/Zv78+WxMrVYzfn5+TExMDKPVajljycvLY4RCIXP27Fk2tmjRIkYoFDK7du3itM3Jyel2fF3X70lmZiYjFAqZuro6Nvbpp58yYrGYkcvlnLb19fWMp6cns2rVKr3fOXfuXM4+aWhoYCZNmsSkpKSwsT/++IMRCoVMfHw8097ezsavXr3KuLi46I1j1apVjFAoNDhu3bL09HRO/Pjx44xQKGQKCgp6vQ8IIf8fXT4mhBiVSqXCiBEjet3+zJkzaGtrg1Qq5ZwRsrS0hFQqRVtbG37++WfOOmPHjkVoaCgnJpFIwDAMpFIpXnrpJTb++uuvA+i8+eVJCxYsgJmZmV5bNzc3iMViNm5mZgaxWIxbt25xxn3//n289dZbaG5uhkKhYH+mTZvGtunKxMQEsbGxnJiPj0+343saDMPg6NGjmDx5Muzs7Djjs7CwgIeHB06fPq23XkxMDGefjB07Fk5OTpxtP3XqFIDOy9tdz76KRCIEBAT0a7xxcXGcz89qvxDyIqPLx4QQo7K0tERra2uv2+vmngkEAr1lutiT898MPabFxsbG4DJra2sAQFNTk946r7zySq/60C3r2odunuKaNWv02urcv3+f89nOzg7m5uac2MiRI7sd39NQKBRoamrC6dOn4evra7DNk5fTAf19ohtj1zuhdTnTXdruysnJCRUVFX0e75O/d9SoUQAGfr8Q8iKjopAQYlQCgQAXLlxAXV2dwQJjIJiamna7zFChA3SeOett2//X/5P9ffTRR5g4caLBNnZ2dr3u19D4noauPz8/PyxdurTX63W3T5617vbNQO8XQl5kVBQSQoxq1qxZuHDhAg4cOIDU1NQe2+sKx+vXr+ud0aqpqeG0GUpeffVVAICFhQX8/PwGtO+ul7/7y9bWFtbW1lCpVAM+Pt2Z1Bs3bujlpuud1DoDsT2EkKdHcwoJIUYVGRkJJycn5ObmGnykDABcuXIF+/btAwD4+/uDz+dj7969UKlUbBuVSoW9e/eCz+fD39/fKGPvi4CAAIwePRo5OTkGL3Gq1WrO9vQFn88HACiVyn6Pz8TEBGFhYaiqqkJpaanBNg8ePOhX34GBgQCA/Px8dHR0sPFr164ZnKeo2x66FEzI4KIzhYQQo7KwsMDOnTuRkJCA5ORkBAQEwM/PDyNHjoRCocC5c+dw+vRpvPPOOwA65/x98MEHyMjIwPz58xEeHg6g85E0t2/fRkZGBqysrAZzkwzi8/nYsGEDkpOTERISgoiICDg6OqK5uRk3btzADz/8gKysLHh7e/e5b7FYDBMTE+zYsQNKpRJ8Ph/29vZwd3fvUz8pKSn47bffsHLlSoSGhsLd3R08Hg937txBRUUFJk2ahPXr1/d5fAKBAFFRUSgsLERcXBxmzpwJhUKB/fv3Y+LEiaiuruacHXR3d8fevXuxdu1aTJ8+HTweD25ubkPyDDAhzzMqCgkhRufo6IjDhw+jsLAQZWVl2LFjB9ra2mBjYwNXV1esX78eYWFhbPuFCxfCzs4Ou3fvZh8S7eLiguzsbAQHBw/WZvRo6tSpKCoqwq5du3DkyBE8fPgQ1tbWcHBwQFxcHEQiUb/6HT9+PD7//HPk5ORg7dq10Gq1CA8P73NRaGVlhYKCAuTm5qK0tBTl5eUwNTXFyy+/DC8vL0RGRvZrfACQnp4OOzs7FBUVYcOGDXByckJ6ejouX76M6upqDB8+nG37xhtvQC6X4/vvv0dpaSk6Ojqwbt06KgoJMbKXGJqlSwghxEgSExNx9uxZ/Prrr726YYcQYjw0p5AQQsiAU6vVerGrV6+ioqICPj4+VBASMgTR5WNCCCED7tChQygpKcH06dNha2uLGzdu4LvvvgOPx8N777032MMjhBhAl48JIYQMuKqqKmzduhVyuRxKpRIjRoyARCLB8uXL4erqOtjDI4QYQEUhIYQQQgihOYWEEEIIIYSKQkIIIYQQAioKCSGEEEIIqCgkhBBCCCGgopAQQgghhICKQkIIIYQQAuA/1g3OGyJaCfMAAAAASUVORK5CYII=\n","text/plain":["<Figure size 720x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"pFjtkyGK3pFX"},"source":["It's interesting to see how many comments are over 512 tokens and the fraction of them that actually contain attack."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pU-TEHwxerZL","executionInfo":{"status":"ok","timestamp":1618744470435,"user_tz":-180,"elapsed":149541,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"d6eb0467-6b1e-4d14-c4b0-482f845cfc20"},"source":["labels"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["rev_id\n","37675        False\n","44816        False\n","49851        False\n","89320        False\n","93890        False\n","             ...  \n","699848324    False\n","699851288    False\n","699857133    False\n","699891012    False\n","699897151    False\n","Name: attack, Length: 115864, dtype: bool"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qr4wd2pT29ZS","executionInfo":{"status":"ok","timestamp":1618744470437,"user_tz":-180,"elapsed":149530,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"b4d3721e-5290-4afa-8531-a91c575d04a9"},"source":["num_truncated = lengths.count(512)\n","num_sentences = len(lengths)\n","print(f\"Percentage of truncated sentences: {num_truncated/num_sentences*100:.2f}%\")\n","\n","labels = train_comments.attack.to_numpy().astype(int)\n","num_pos = 0\n","num_neg = 0\n","for i, l in enumerate(lengths):\n","  if l == 512:\n","    if labels[i] == 0:\n","      num_neg += 1\n","    else:\n","      num_pos += 1\n","print(f\"Percentage of truncated sentences that actually contained attack: {num_pos/num_truncated*100:.2f}%\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Percentage of truncated sentences: 2.40%\n","Percentage of truncated sentences that actually contained attack: 12.11%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xeaMfiRmf2P0"},"source":["It's safe to assume that 512 tokens is enough: we won't lose much texts. Actually we'll go even further and use maximum length of 128 as and \"elbow\" point on the plotted distribution."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKHV5kzxd3or","executionInfo":{"status":"ok","timestamp":1618744470839,"user_tz":-180,"elapsed":149917,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"4b98aac5-cc61-4a33-b044-c2a570f68b82"},"source":["MAX_LEN = 128\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', padding='post', truncating='post', value=0)\n","print(f\"input_ids.shape: {input_ids.shape}\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["input_ids.shape: (69526, 128)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2_IqMr-yjjyG"},"source":["The attention mask simply makes it explicit which tokens are actual words versus which are padding. \n","\n","The BERT vocabulary does not use the ID 0, so if a token ID is 0, then it's padding, and otherwise it's a real token."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"up2J7UJ_jUc5","executionInfo":{"status":"ok","timestamp":1618744476779,"user_tz":-180,"elapsed":155843,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"49fc067f-fea5-42e8-9561-7bc2cf416b36"},"source":["attention_masks = []\n","for sent in input_ids:\n","  att_msk = [int(token_id > 0) for token_id in sent]\n","  attention_masks.append(att_msk)\n","attention_masks = np.array(attention_masks)\n","print(f\"attention_masks.shape: {attention_masks.shape}\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["attention_masks.shape: (69526, 128)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PB6YcxJWlLVc"},"source":["Next step would be splitting our dataset into train/validation, converting them into data type expected by PyTorch and creating DataLoader that will feed batches of dat into the GPU's memory."]},{"cell_type":"code","metadata":{"id":"1bfmCmNAkErN","executionInfo":{"status":"ok","timestamp":1618744476781,"user_tz":-180,"elapsed":155843,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}}},"source":["train_input_ids, validation_input_ids, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2021, test_size=.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2021, test_size=.1)\n","\n","train_input_ids = torch.tensor(train_input_ids)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","\n","validation_input_ids = torch.tensor(validation_input_ids)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\n","\n","batch_sz = 64\n","train_data = TensorDataset(train_input_ids, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_sz)\n","\n","validation_data = TensorDataset(validation_input_ids, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_sz)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dise50Hjn4YO"},"source":["# BERT Fine-tuning\n","\n","For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. \n","\n","Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.  \n","\n","Here is the current list of classes provided for fine-tuning:\n","* BertModel\n","* BertForPreTraining\n","* BertForMaskedLM\n","* BertForNextSentencePrediction\n","* **BertForSequenceClassification** - The one we'll use.\n","* BertForTokenClassification\n","* BertForQuestionAnswering"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f3e20c3612db48d89fe963c31cb63af5","ee6a848c245a41f4906d7ada4871b0d7","fdc518c0d369479db0aeacbcd6caa6dc","3bbe9898c9ef483da8201d0347d4e743","e49ec6ea6c584962b80d530ac9ae4c60","78e8806dc69b45ef8ab238be818e855d","094bcbdec0ad435b9183e6ae72fe0b48","5199e67f28d4457b944edefecedb905e","d2517ac8066d427c81873d3493396bc9","8bd4d566b73940c78792d961474bb392","33026cdf929849839b090b3e5ad98b07","c8ecd217beb64722912a7f755f7c9a30","4b8afe63824b49ccb935c84572c1844f","b98705b96bce401aa7413e18feb4a1b5","11c176c81d184c36939b070d188c8310","89175c0046de4a91aabccbe365f9f4a2"]},"id":"PJZL8H5pnQun","executionInfo":{"status":"ok","timestamp":1618744491914,"user_tz":-180,"elapsed":170961,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"8ae188ba-60a7-410d-e41b-b0d2c7bcede6"},"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3e20c3612db48d89fe963c31cb63af5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2517ac8066d427c81873d3493396bc9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"VjvqzHdbnUh7","executionInfo":{"status":"ok","timestamp":1618744491916,"user_tz":-180,"elapsed":170958,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}}},"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dq-uVL5c9_H5"},"source":["Let's define some gelper functions"]},{"cell_type":"code","metadata":{"id":"hUmU-Yua99rI","executionInfo":{"status":"ok","timestamp":1618744491917,"user_tz":-180,"elapsed":170956,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.mean(pred_flat == labels_flat)\n","\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acKTr9Vg-Y6H"},"source":["Time to actually train our model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6XSxFV5-P-t","executionInfo":{"status":"ok","timestamp":1618747422425,"user_tz":-180,"elapsed":3101450,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"8f534de5-03fe-4c57-c32a-c615e7ce7a9e"},"source":["# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print(f'======== Epoch {epoch_i} / {epochs} ========')\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 100 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # In PyTorch, calling `model` will in turn call the model's `forward` \n","        # function and pass down the arguments. The `forward` function is \n","        # documented here: \n","        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n","        # The results are returned in a results object, documented here:\n","        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n","    print(f\"  Training epcoh took: {format_time(time.time() - t0)}\")\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_accuracy = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","        # output values prior to applying an activation function like the \n","        # softmax.\n","        loss = result.loss\n","        logits = result.logits\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(f\"  Accuracy: {eval_accuracy/nb_eval_steps:.2f}\")\n","    print(f\"  Validation took: {format_time(time.time() - t0)}\")\n","\n","print(\"\")\n","print(\"Training complete!\")\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 0 / 4 ========\n","Training...\n","  Batch   100  of    978.    Elapsed: 0:01:12.\n","  Batch   200  of    978.    Elapsed: 0:02:24.\n","  Batch   300  of    978.    Elapsed: 0:03:37.\n","  Batch   400  of    978.    Elapsed: 0:04:49.\n","  Batch   500  of    978.    Elapsed: 0:06:01.\n","  Batch   600  of    978.    Elapsed: 0:07:13.\n","  Batch   700  of    978.    Elapsed: 0:08:25.\n","  Batch   800  of    978.    Elapsed: 0:09:38.\n","  Batch   900  of    978.    Elapsed: 0:10:50.\n","\n","  Average training loss: 0.22\n","  Training epcoh took: 0:11:46\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation took: 0:00:26\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch   100  of    978.    Elapsed: 0:01:12.\n","  Batch   200  of    978.    Elapsed: 0:02:24.\n","  Batch   300  of    978.    Elapsed: 0:03:37.\n","  Batch   400  of    978.    Elapsed: 0:04:49.\n","  Batch   500  of    978.    Elapsed: 0:06:01.\n","  Batch   600  of    978.    Elapsed: 0:07:13.\n","  Batch   700  of    978.    Elapsed: 0:08:26.\n","  Batch   800  of    978.    Elapsed: 0:09:38.\n","  Batch   900  of    978.    Elapsed: 0:10:50.\n","\n","  Average training loss: 0.14\n","  Training epcoh took: 0:11:47\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation took: 0:00:26\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch   100  of    978.    Elapsed: 0:01:12.\n","  Batch   200  of    978.    Elapsed: 0:02:25.\n","  Batch   300  of    978.    Elapsed: 0:03:37.\n","  Batch   400  of    978.    Elapsed: 0:04:49.\n","  Batch   500  of    978.    Elapsed: 0:06:02.\n","  Batch   600  of    978.    Elapsed: 0:07:14.\n","  Batch   700  of    978.    Elapsed: 0:08:26.\n","  Batch   800  of    978.    Elapsed: 0:09:39.\n","  Batch   900  of    978.    Elapsed: 0:10:51.\n","\n","  Average training loss: 0.12\n","  Training epcoh took: 0:11:47\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation took: 0:00:26\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch   100  of    978.    Elapsed: 0:01:12.\n","  Batch   200  of    978.    Elapsed: 0:02:25.\n","  Batch   300  of    978.    Elapsed: 0:03:37.\n","  Batch   400  of    978.    Elapsed: 0:04:49.\n","  Batch   500  of    978.    Elapsed: 0:06:02.\n","  Batch   600  of    978.    Elapsed: 0:07:14.\n","  Batch   700  of    978.    Elapsed: 0:08:27.\n","  Batch   800  of    978.    Elapsed: 0:09:39.\n","  Batch   900  of    978.    Elapsed: 0:10:51.\n","\n","  Average training loss: 0.11\n","  Training epcoh took: 0:11:48\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation took: 0:00:26\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x8JxNfO_ENtP"},"source":["# Evaluation on the test set\n","\n","At first we will do all the needed prerocessing for the test set. Then we will do predictions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60kguVv-EQ-f","executionInfo":{"status":"ok","timestamp":1618747463562,"user_tz":-180,"elapsed":3142572,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"7e760fe8-ff7c-4757-d949-fda4f605e520"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","test_input_ids = []\n","\n","# For every sentence...\n","for sen in test_comments.comment:\n","    \n","    # Report progress.\n","    if ((len(input_ids) % 20000) == 0):\n","        print(f'  Read {len(input_ids)} comments.')\n","    \n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sen,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = MAX_LEN,          # Truncate all sentences.                        \n","                   )\n","    \n","    # Add the encoded sentence to the list.\n","    test_input_ids.append(encoded_sent)\n","\n","print('DONE.')\n","print('')\n","print(f'{len(test_input_ids):>10,} test comments')\n","\n","# Also retrieve the labels as a list.\n","\n","# Get the labels from the DataFrame, and convert from booleans to ints.\n","test_labels = test_comments.attack.to_numpy().astype(int)\n","\n","print(f'{np.sum(test_labels):>10,} positive (contains attack)')\n","print(f'{len(test_labels) - np.sum(test_labels):>10,} negative (not an attack)')\n","\n","# Pad our input tokens\n","test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, \n","                               dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","test_attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in test_input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  test_attention_masks.append(seq_mask) \n","\n","# Convert to tensors.\n","test_inputs = torch.tensor(test_input_ids)\n","test_masks = torch.tensor(test_attention_masks)\n","test_labels = torch.tensor(test_labels)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["DONE.\n","\n","    23,178 test comments\n","     2,756 positive (contains attack)\n","    20,422 negative (not an attack)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZlPRoY1FOyc","executionInfo":{"status":"ok","timestamp":1618747552140,"user_tz":-180,"elapsed":3231136,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"a40adc95-9baa-4462-c338-ef86714ad03e"},"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(test_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Measure elapsed time.\n","t0 = time.time()\n","\n","# Predict \n","for (step, batch) in enumerate(test_dataloader):\n","    \n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","  \n","    # Progress update every 100 batches.\n","    if step % 100 == 0 and not step == 0:\n","        # Calculate elapsed time in minutes.\n","        elapsed = format_time(time.time() - t0)\n","        \n","        # Report progress.\n","        print(f'  Batch {step:>5,}  of  {len(test_dataloader):>5,}.    Elapsed: {elapsed}.')\n","\n","\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","  \n","    # Telling the model not to compute or store gradients, saving memory and \n","    # speeding up prediction\n","    with torch.no_grad():\n","        # Forward pass, calculate logit predictions.\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask,\n","                       return_dict=True)\n","\n","    logits = result.logits\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","  \n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","print('    DONE.')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Predicting labels for 23,178 test sentences...\n","  Batch   100  of    725.    Elapsed: 0:00:12.\n","  Batch   200  of    725.    Elapsed: 0:00:24.\n","  Batch   300  of    725.    Elapsed: 0:00:37.\n","  Batch   400  of    725.    Elapsed: 0:00:49.\n","  Batch   500  of    725.    Elapsed: 0:01:01.\n","  Batch   600  of    725.    Elapsed: 0:01:13.\n","  Batch   700  of    725.    Elapsed: 0:01:26.\n","    DONE.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WvxaWkZAF-Oa"},"source":["The accuracy metric chosen by the authors for this dataset is the \"ROC AUC\" (Receiver Operating Characteristic, Area Under the Curve) rather than straight accuracy (number right / total examples).\n","\n","*ROC AUC*\n","\n","To illustrate the purpose of this metric, let's say that you were going to deploy this comment classifier on your website to automatically flag bad comments. In order to do that, you would have to make a decision about how confident you needed the classifier to be before flagging a comment as a personal attack.\n","* If it was critical that no bad comments be missed, then you might choose to set a fairly low threshold, and then have a human review what the classifier flags. This would help ensure that bad comments would be caught, but at the cost of getting more false positives that the moderator would have to deal with.\n","* If it wasn't critical to catch them all, and you wanted as few as possible to manually review, then you might you set a higher threshold so that you don't have as many flagged comments to review (at the risk of missing some attacks). \n","\n","The ROC AUC takes into account the fact that you can adjust the threshold to trade off false positives and false negatives, and yields a score which tries to capture overall accuracy independent of where you choose to put that threshold."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zv4Wx6svF-3Y","executionInfo":{"status":"ok","timestamp":1618747552142,"user_tz":-180,"elapsed":3231126,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"9eefaf9d-33e8-4184-8b2c-04848f33bcb6"},"source":["# Combine the results across the batches.\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Use the model output for label 1 as our predictions.\n","p1 = predictions[:,1]\n","\n","# Calculate the ROC AUC.\n","auc = roc_auc_score(true_labels, p1)\n","\n","print(f'Test ROC AUC: {auc:.3f}')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Test ROC AUC: 0.964\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rmC0CNcbGMMC"},"source":["# Saving the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVyRoFKOGLj8","executionInfo":{"status":"ok","timestamp":1618747553289,"user_tz":-180,"elapsed":3232261,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"b2f90cef-16c4-4cb3-b6fc-1fa63fc6a8a5"},"source":["# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","output_dir = './model_save/'\n","\n","# Create output directory if needed\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('./model_save/tokenizer_config.json',\n"," './model_save/special_tokens_map.json',\n"," './model_save/vocab.txt',\n"," './model_save/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"3M20sQ16HDLo"},"source":["# Backing Up to Google Drive"]},{"cell_type":"code","metadata":{"id":"zfkWW6VqHErX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618747575040,"user_tz":-180,"elapsed":3254000,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"15f36692-ed1f-4047-8991-294c9652adc3"},"source":["drive.mount('/content/drive')\n","\n","gdrive_path = \"./drive/MyDrive/\"\n","\n","os.makedirs(gdrive_path, exist_ok=True)\n","\n","# Copy the model files to a directory in your Google Drive.\n","!cp -r ./model_save/ \"./drive/My Drive/\""],"execution_count":28,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OjuFbGY0HqTV"},"source":["# Part III - Semantic Similarity\n","\n","In Part III, we'll walk through an example of using BERT to measure the \"semantic similarity\" of two pieces of text. \"Semantic similarity\" refers to how close in meaning the pieces of text are.\n","\n","As a demonstration of this capability, we'll implement code to compare an arbitrary piece of input text (it could be from the dataset or one we supply ourselves) with all of the comments in the dataset and return the comments that are most similar in meaning to the input text.\n","\n","In this section, we will use our fine-tuned BERT model to extract sentence embeddings for all of the training comments.\n","\n","To create these embeddings, we'll simply run the comments through the model the same way we did for training, but we'll take the embedding corresponding to the `[CLS]` token from the final BERT Transformer layer. It's the blue rectangle at the top left of the following illustration:\n","\n","[![Illustration of CLS token purpose](https://drive.google.com/uc?export=view&id=1ck4mvGkznVJfW3hv6GUqcdGepVTOx7HE)](https://drive.google.com/uc?export=view&id=1ck4mvGkznVJfW3hv6GUqcdGepVTOx7HE)"]},{"cell_type":"markdown","metadata":{"id":"udGa79WpECdV"},"source":["#  Modifying Our Model to Output Embeddings\n","\n","Important: Before we can use our fine-tuned model for extracting embeddings, we first need to reload the model from disk with the `output_hidden_states` flag set to `True`."]},{"cell_type":"code","metadata":{"id":"kVPeFNhmHtsH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618747578132,"user_tz":-180,"elapsed":3257090,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"8c117d7d-e9cf-48e1-dda9-5362d4c80a68"},"source":["# !cp -r \"./drive/My Drive/model_save/\" ./model_save/\n","\n","# Load our fine-tuned model, and configure it to return the \"hidden states\", \n","# from which we will be taking our text embeddings.\n","model = BertForSequenceClassification.from_pretrained(\n","    output_dir,\n","    output_hidden_states = True, # Whether the model returns all hidden-states.\n",") \n","\n","# Load the tokenizer.\n","tokenizer = BertTokenizer.from_pretrained(output_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"Z3QvR0ZkFpqT"},"source":["Here we'll define a function which can take an arbitrary piece of text and use our fine-tuned BERT model to compute a feature vector (aka \"embedding\") for the text."]},{"cell_type":"code","metadata":{"id":"Qs72B_olFuvJ","executionInfo":{"status":"ok","timestamp":1618747578134,"user_tz":-180,"elapsed":3257089,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}}},"source":["def text_to_embedding(tokenizer, model, in_text):\n","    '''\n","    Uses the provided BERT `model` and `tokenizer` to generate a vector \n","    representation of the input string, `in_text`.\n","\n","    Returns the vector stored as a numpy ndarray.\n","    '''\n","\n","    # ===========================\n","    #    STEP 1: Tokenization\n","    # ===========================\n","\n","    MAX_LEN = 128\n","\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Truncate the sentence to MAX_LEN if necessary.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end. (After truncating!)\n","    #   (4) Map tokens to their IDs.\n","    input_ids = tokenizer.encode(\n","                        in_text,                    # Sentence to encode.\n","                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n","                        max_length = MAX_LEN,       # Truncate all sentences.                        \n","                   )    \n","\n","    # Pad our input tokens. Truncation was handled above by the `encode`\n","    # function, which also makes sure that the `[SEP]` token is placed at the\n","    # end *after* truncating.\n","    # Note: `pad_sequences` expects a list of lists, but we only have one\n","    # piece of text, so we surround `input_ids` with an extra set of brackets.\n","    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", \n","                              truncating=\"post\", padding=\"post\")\n","    \n","    # Remove the outer list.\n","    input_ids = results[0]\n","\n","    # Create attention masks    \n","    attn_mask = [int(i>0) for i in input_ids]\n","    \n","    # Cast to tensors.\n","    input_ids = torch.tensor(input_ids)\n","    attn_mask = torch.tensor(attn_mask)\n","\n","    # Add an extra dimension for the \"batch\" (even though there is only one \n","    # input in this batch.)\n","    input_ids = input_ids.unsqueeze(0)\n","    attn_mask = attn_mask.unsqueeze(0)\n","\n","    # ===========================\n","    #    STEP 2: BERT Model\n","    # ===========================\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Copy the inputs to the GPU\n","    # Note -- I got stuck here for a while because I didn't assign the result \n","    # back to the variable! Geez!\n","    input_ids = input_ids.to(device)\n","    attn_mask = attn_mask.to(device)\n","    \n","    # Telling the model not to build the backwards graph will make this \n","    # a little quicker.\n","    with torch.no_grad():        \n","\n","        # Forward pass, return hidden states and predictions.\n","        # This will return the logits rather than the loss because we have\n","        # not provided labels.\n","        logits, encoded_layers = model(\n","                                    input_ids = input_ids, \n","                                    token_type_ids = None, \n","                                    attention_mask = attn_mask,\n","                                    return_dict=False)\n","        \n","    # Retrieve our sentence embedding--take the `[CLS]` embedding from the final\n","    # layer.\n","    layer_i = 12 # The last BERT layer before the classifier.\n","    batch_i = 0 # Only one input in the batch.\n","    token_i = 0 # The first token, corresponding to [CLS]\n","        \n","    # Grab the embedding.\n","    vec = encoded_layers[layer_i][batch_i][token_i]\n","\n","    # Move to the CPU and convert to numpy ndarray.\n","    vec = vec.detach().cpu().numpy()\n","\n","    return(vec)\n"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQ4WgMvjF0av","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618747578135,"user_tz":-180,"elapsed":3257087,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"df09df81-5eec-43ff-a010-f3b2990dc0bf"},"source":["# Get the text from one of the comments.\n","input_text = comments.iloc[10].comment\n","\n","# Use the BERT model and tokenizer to generate an embedding for `input_text`.\n","vec = text_to_embedding(tokenizer, model, input_text)\n","\n","print(f'\\nDone. Embedding shape:{vec.shape}')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Done. Embedding shape:(768,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jVcJ_f-yIRVF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618748789011,"user_tz":-180,"elapsed":4467962,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"50780ce8-b754-4c81-f152-882e4efac309"},"source":["# Track the time.\n","t0 = time.time()\n","\n","# Store the set of embeddings.\n","embeddings = []\n","\n","num_comments = len(comments)\n","\n","print(f'Generating sentence embeddings for all {num_comments} comments...')\n","\n","row_num = 0\n","\n","# For each row of the dataframe...\n","for index, row in comments.iterrows():\n","\n","    # Progress update every 2,000 comments.\n","    if row_num % 2000 == 0 and not row_num == 0:\n","\n","        # Calculate elapsed time and format it.\n","        elapsed = format_time(time.time() - t0)\n","        \n","        # Calculate the time remaining based on our progress.\n","        rows_per_sec = (time.time() - t0) / row_num\n","        remaining_sec = rows_per_sec * (num_comments - row_num)\n","        remaining = format_time(remaining_sec)\n","\n","        # Report progress.\n","        print(f'  Comment {row_num:>7,}  of  {num_comments:>7,}.    Elapsed: {elapsed}. Remaining: {remaining}')\n","\n","    # Vectorize this comment.\n","    vec = text_to_embedding(tokenizer, model, row.comment)\n","\n","    # Store the embeddings.\n","    embeddings.append(vec) \n","\n","    row_num += 1"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Generating sentence embeddings for all 115864 comments...\n","  Comment   2,000  of  115,864.    Elapsed: 0:00:22. Remaining: 0:21:06\n","  Comment   4,000  of  115,864.    Elapsed: 0:00:44. Remaining: 0:20:42\n","  Comment   6,000  of  115,864.    Elapsed: 0:01:06. Remaining: 0:20:08\n","  Comment   8,000  of  115,864.    Elapsed: 0:01:27. Remaining: 0:19:36\n","  Comment  10,000  of  115,864.    Elapsed: 0:01:48. Remaining: 0:19:04\n","  Comment  12,000  of  115,864.    Elapsed: 0:02:09. Remaining: 0:18:35\n","  Comment  14,000  of  115,864.    Elapsed: 0:02:30. Remaining: 0:18:09\n","  Comment  16,000  of  115,864.    Elapsed: 0:02:50. Remaining: 0:17:43\n","  Comment  18,000  of  115,864.    Elapsed: 0:03:12. Remaining: 0:17:22\n","  Comment  20,000  of  115,864.    Elapsed: 0:03:33. Remaining: 0:16:59\n","  Comment  22,000  of  115,864.    Elapsed: 0:03:54. Remaining: 0:16:38\n","  Comment  24,000  of  115,864.    Elapsed: 0:04:15. Remaining: 0:16:15\n","  Comment  26,000  of  115,864.    Elapsed: 0:04:36. Remaining: 0:15:53\n","  Comment  28,000  of  115,864.    Elapsed: 0:04:57. Remaining: 0:15:32\n","  Comment  30,000  of  115,864.    Elapsed: 0:05:17. Remaining: 0:15:08\n","  Comment  32,000  of  115,864.    Elapsed: 0:05:38. Remaining: 0:14:47\n","  Comment  34,000  of  115,864.    Elapsed: 0:05:59. Remaining: 0:14:25\n","  Comment  36,000  of  115,864.    Elapsed: 0:06:20. Remaining: 0:14:03\n","  Comment  38,000  of  115,864.    Elapsed: 0:06:42. Remaining: 0:13:43\n","  Comment  40,000  of  115,864.    Elapsed: 0:07:02. Remaining: 0:13:21\n","  Comment  42,000  of  115,864.    Elapsed: 0:07:23. Remaining: 0:12:59\n","  Comment  44,000  of  115,864.    Elapsed: 0:07:44. Remaining: 0:12:37\n","  Comment  46,000  of  115,864.    Elapsed: 0:08:05. Remaining: 0:12:16\n","  Comment  48,000  of  115,864.    Elapsed: 0:08:26. Remaining: 0:11:55\n","  Comment  50,000  of  115,864.    Elapsed: 0:08:48. Remaining: 0:11:35\n","  Comment  52,000  of  115,864.    Elapsed: 0:09:08. Remaining: 0:11:14\n","  Comment  54,000  of  115,864.    Elapsed: 0:09:30. Remaining: 0:10:53\n","  Comment  56,000  of  115,864.    Elapsed: 0:09:51. Remaining: 0:10:32\n","  Comment  58,000  of  115,864.    Elapsed: 0:10:12. Remaining: 0:10:10\n","  Comment  60,000  of  115,864.    Elapsed: 0:10:32. Remaining: 0:09:49\n","  Comment  62,000  of  115,864.    Elapsed: 0:10:53. Remaining: 0:09:27\n","  Comment  64,000  of  115,864.    Elapsed: 0:11:14. Remaining: 0:09:06\n","  Comment  66,000  of  115,864.    Elapsed: 0:11:34. Remaining: 0:08:44\n","  Comment  68,000  of  115,864.    Elapsed: 0:11:55. Remaining: 0:08:23\n","  Comment  70,000  of  115,864.    Elapsed: 0:12:16. Remaining: 0:08:02\n","  Comment  72,000  of  115,864.    Elapsed: 0:12:37. Remaining: 0:07:41\n","  Comment  74,000  of  115,864.    Elapsed: 0:12:57. Remaining: 0:07:20\n","  Comment  76,000  of  115,864.    Elapsed: 0:13:18. Remaining: 0:06:59\n","  Comment  78,000  of  115,864.    Elapsed: 0:13:38. Remaining: 0:06:37\n","  Comment  80,000  of  115,864.    Elapsed: 0:13:59. Remaining: 0:06:16\n","  Comment  82,000  of  115,864.    Elapsed: 0:14:20. Remaining: 0:05:55\n","  Comment  84,000  of  115,864.    Elapsed: 0:14:41. Remaining: 0:05:34\n","  Comment  86,000  of  115,864.    Elapsed: 0:15:02. Remaining: 0:05:13\n","  Comment  88,000  of  115,864.    Elapsed: 0:15:22. Remaining: 0:04:52\n","  Comment  90,000  of  115,864.    Elapsed: 0:15:43. Remaining: 0:04:31\n","  Comment  92,000  of  115,864.    Elapsed: 0:16:04. Remaining: 0:04:10\n","  Comment  94,000  of  115,864.    Elapsed: 0:16:24. Remaining: 0:03:49\n","  Comment  96,000  of  115,864.    Elapsed: 0:16:45. Remaining: 0:03:28\n","  Comment  98,000  of  115,864.    Elapsed: 0:17:06. Remaining: 0:03:07\n","  Comment 100,000  of  115,864.    Elapsed: 0:17:27. Remaining: 0:02:46\n","  Comment 102,000  of  115,864.    Elapsed: 0:17:47. Remaining: 0:02:25\n","  Comment 104,000  of  115,864.    Elapsed: 0:18:09. Remaining: 0:02:04\n","  Comment 106,000  of  115,864.    Elapsed: 0:18:29. Remaining: 0:01:43\n","  Comment 108,000  of  115,864.    Elapsed: 0:18:50. Remaining: 0:01:22\n","  Comment 110,000  of  115,864.    Elapsed: 0:19:11. Remaining: 0:01:01\n","  Comment 112,000  of  115,864.    Elapsed: 0:19:31. Remaining: 0:00:40\n","  Comment 114,000  of  115,864.    Elapsed: 0:19:52. Remaining: 0:00:19\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b28FBiBIKW6b"},"source":["Since creating those embeddings was such a lengthy process, let's be sure to save the embeddings to disk in case we want to reload them another time."]},{"cell_type":"code","metadata":{"id":"nxq_QAvKIf65","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618748789014,"user_tz":-180,"elapsed":4467961,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"e36605f2-273b-4711-a9ec-e028b5611ad6"},"source":["# Convert the list of vectors into a 2D array.\n","vecs = np.stack(embeddings)\n","\n","vecs.shape"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(115864, 768)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"ne0KWpeJKLru","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618748790786,"user_tz":-180,"elapsed":4469731,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"179253ae-5156-4b22-c1eb-9fa8242d0e0d"},"source":["# Use numpy to write out the matrix of embeddings.\n","print(\"Saving embeddings to: ./model_save/embeddings.npy\")\n","np.save('./model_save/embeddings.npy', vecs)\n","\n","# Copy the embeddings to a directory in your Google Drive.\n","#!cp -r ./model_save/embeddings.npy \"./drive/My Drive/model_save/\""],"execution_count":34,"outputs":[{"output_type":"stream","text":["Saving embeddings to: ./model_save/embeddings.npy\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dFQFlDJGKql1"},"source":["# Semantic Similarity Search\n","\n","Now that we have our comments all vectorized, we are ready to make them \"searchable\".\n","\n","We do this using a technique called \"k-Nearest Neighbor Search\" or \"k-NN\". Simply put, we use a distance metric such as Euclidean distance, calculate that distance between our \"query\" vector and all of the vectors to be searched, then sort the distances to find the closest matches.\n","\n","All of those distance calculations can make k-NN search computationally expensive and slow. There are a number of libraries out there for accelerating k-NN using carefully optimized code and / or approximation techniques.\n","\n","I personally like the FAISS (Facebook AI Similarity Search) library, in part because it has a really excellent GPU implementation.\n","\n","Using the GPU, we can perform \"brute-force\" k-NN search (meaning no approximation techniques which compromise on accuracy) on this dataset quickly."]},{"cell_type":"code","metadata":{"id":"LSBp79YbKuM-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618748805717,"user_tz":-180,"elapsed":4484659,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"283f7774-bb02-4fd2-8683-71e0004df682"},"source":["!pip install faiss\n","!pip install faiss-gpu"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Collecting faiss\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/2e/dc5697e9ff6f313dcaf3afe5ca39d7d8334114cbabaed069d0026bbc3c61/faiss-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (4.7MB)\n","\u001b[K     |████████████████████████████████| 4.7MB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.19.5)\n","Installing collected packages: faiss\n","Successfully installed faiss-1.5.3\n","Collecting faiss-gpu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/36/383911b8edf8c29cb7e9e8aee4e6b69b0f36c52237e3a06ce64a9551ef22/faiss_gpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl (89.4MB)\n","\u001b[K     |████████████████████████████████| 89.4MB 61kB/s \n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"__rTpeAfK0Vl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618748806228,"user_tz":-180,"elapsed":4485168,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"75737781-d61a-4515-9c8c-44359ed4421f"},"source":["import faiss\n","\n","# Build a flat (CPU) index\n","cpu_index = faiss.IndexFlatL2(vecs.shape[1])\n","\n","# Use 1 GPU.\n","n_gpu = 1\n","\n","# Print the number of available GPUs. \n","print(f'Number of available GPUs: {faiss.get_num_gpus()}    Using: {n_gpu}')\n","\n","# If using multiple GPUs, enable sharding so that the dataset is divided across \n","# the GPUs rather than replicated.\n","co = faiss.GpuMultipleClonerOptions()\n","co.shard = True\n","\n","# Make it into a gpu index\n","gpu_index = faiss.index_cpu_to_all_gpus(cpu_index, co=co, ngpu=n_gpu)\n","\n","# Add vecs to our GPU index\n","print('Adding dataset to index...')\n","t0 = time.time()    \n","\n","gpu_index.add(vecs)\n","\n","elapsed = time.time() - t0\n","print(f'Building index took {elapsed:.2f} seconds')"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Number of available GPUs: 1    Using: 1\n","Adding dataset to index...\n","Building index took 0.08 seconds\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jn-l-F7mMuob"},"source":["Now let's try taking one of the comments from the dataset, and searching for the most semantically similar comments."]},{"cell_type":"code","metadata":{"id":"1WoJdVQsMyPC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618748806229,"user_tz":-180,"elapsed":4485166,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"d6ac125a-157a-4f38-824e-9734afc3193c"},"source":["# Comment number 4 is short and sweet.\n","print('==== Input Comment =====')\n","print('Comment #4:')\n","print(comments.iloc[4].comment)\n","\n","# Let's find the 5 most similar comments.\n","D, I = gpu_index.search(vecs[4].reshape(1, 768), k=5) \n","\n","print('')\n","print('==== Top 5 Results ====')\n","\n","# For each result...\n","for i in range(I.shape[1]):\n","\n","    # Look up the comment row number for this result.\n","    result_i = I[0, i]\n","\n","    # Look up the text for this comment.\n","    text = comments.iloc[result_i].comment\n","\n","    print('Comment #{:,}:'.format(result_i))\n","    print('L2 Distance: %.2f' % D[0, i])\n","    print('\"' + text + '\"')\n","    print('')"],"execution_count":37,"outputs":[{"output_type":"stream","text":["==== Input Comment =====\n","Comment #4:\n","This page will need disambiguation. \n","\n","==== Top 5 Results ====\n","Comment #4:\n","L2 Distance: 0.00\n","\"This page will need disambiguation. \"\n","\n","Comment #100,779:\n","L2 Distance: 0.57\n","\" Description on image page is confusing about this.   \"\n","\n","Comment #8,975:\n","L2 Distance: 0.64\n","\"For more details on this topic, see . \"\n","\n","Comment #101,805:\n","L2 Distance: 0.64\n","\"I need help to improve this page.\"\n","\n","Comment #105,064:\n","L2 Distance: 0.66\n","\"still feel it belongs, feel free to explain.   \"\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FxCwyVNzM0-V"},"source":["Using our `text_to_embedding` function, we can also define new text to use as our query. Let's try writing a new sentence that's similar in meaning to comment #4, but uses different language. The word \"disambiguate\" means \"remove uncertainty of meaning from\", so I've written \"The meaning of this page needs to be clarified.\"\n","\n"]},{"cell_type":"code","metadata":{"id":"eTtUy-ucM3SY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618748806771,"user_tz":-180,"elapsed":4485707,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"bdd699ea-9ff2-4ca5-ed36-fe530ca8bdfd"},"source":["query_text = \"The meaning of this page needs to be clarified.\"\n","\n","# Vectorize a new piece of text.\n","query_vec = text_to_embedding(tokenizer, model, query_text)\n","\n","# Let's find the 5 most similar comments.\n","D, I = gpu_index.search(query_vec.reshape(1, 768), k=5) \n","\n","print('')\n","print('==== Top 5 Results ====')\n","\n","# For each result...\n","for i in range(I.shape[1]):\n","\n","    # Look up the comment row number for this result.\n","    result_i = I[0, i]\n","\n","    # Look up the text for this comment.\n","    text = comments.iloc[result_i].comment\n","\n","    print('Comment #{:,}:'.format(result_i))\n","    print('L2 Distance: %.2f' % D[0, i])\n","    print('\"' + text + '\"')\n","    print('')"],"execution_count":38,"outputs":[{"output_type":"stream","text":["\n","==== Top 5 Results ====\n","Comment #84,400:\n","L2 Distance: 1.01\n","\"  What does Mexico have to do with it?   \"\n","\n","Comment #74,594:\n","L2 Distance: 1.05\n","\"Everything seems back to normal now, thanks.     \"\n","\n","Comment #54,132:\n","L2 Distance: 1.06\n","\"  :Shouldn't it be Lone instead of lone?   \"\n","\n","Comment #89,523:\n","L2 Distance: 1.06\n","\"accusation of sockpuppetery happened.}}\"\n","\n","Comment #42,819:\n","L2 Distance: 1.06\n","\"Do not put your email or phone number on article pages.   \"\n","\n"],"name":"stdout"}]}]}