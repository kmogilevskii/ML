{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ALBERT_Quora_Dataset.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNUwnTeL7zlfkWYKJpfGQDN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8a68029c4215447ab3cd44404ee6f218":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bb3455c16ac54cb88c0b50066c3b7f15","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c40d7e8d87fb46f3b1270f55afa02c2c","IPY_MODEL_a8b9048ca23948558ea714eeb8274aca"]}},"bb3455c16ac54cb88c0b50066c3b7f15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c40d7e8d87fb46f3b1270f55afa02c2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cb37b31af8424e4da46fadb3d68dbf10","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":760289,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":760289,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_27d8c380204e4be88dc53f1ef2af7a53"}},"a8b9048ca23948558ea714eeb8274aca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c7523e3922ad44c2afb39d4b2b3a7e98","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 760k/760k [02:06&lt;00:00, 6.02kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb28c862524c4a88842cf632cfd961ea"}},"cb37b31af8424e4da46fadb3d68dbf10":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"27d8c380204e4be88dc53f1ef2af7a53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7523e3922ad44c2afb39d4b2b3a7e98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eb28c862524c4a88842cf632cfd961ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"881c00c2cbf647c6b40c87dca8af3fb0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e11bc33450a14bb68ed0cb7a95bca433","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_268fc0428a4e4cdb83429e2a858fe844","IPY_MODEL_ceb65a0a048845208c65668092ff8c0b"]}},"e11bc33450a14bb68ed0cb7a95bca433":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"268fc0428a4e4cdb83429e2a858fe844":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_875d65445d854d3d8edb49b6ca0fdf6b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1312669,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1312669,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_798013c680c2402ebffb91eccd7c31ec"}},"ceb65a0a048845208c65668092ff8c0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_79a0b25175ef45e0b39b48b5a0dc5952","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.31M/1.31M [02:04&lt;00:00, 10.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_76ad480b73304d10aac48da2b583a96c"}},"875d65445d854d3d8edb49b6ca0fdf6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"798013c680c2402ebffb91eccd7c31ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79a0b25175ef45e0b39b48b5a0dc5952":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"76ad480b73304d10aac48da2b583a96c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea1b3289e98c4d7aa278ce2c32929c4a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a1b07eec7de24cb28700c500e493fe85","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0eb3b75c8839494d810d87b616ca9836","IPY_MODEL_9b9e2d2cc3304787936ca57feef1f68c"]}},"a1b07eec7de24cb28700c500e493fe85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0eb3b75c8839494d810d87b616ca9836":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f4960a109f7149df872a1c3e656d85fe","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":706,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":706,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_521b2486bd8d4a4c955c9dc4a5f3c90f"}},"9b9e2d2cc3304787936ca57feef1f68c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7376fde0bbad42c0b13a620a7e455847","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 706/706 [04:49&lt;00:00, 2.44B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e332a4287d4411dac221e16f2145f82"}},"f4960a109f7149df872a1c3e656d85fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"521b2486bd8d4a4c955c9dc4a5f3c90f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7376fde0bbad42c0b13a620a7e455847":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6e332a4287d4411dac221e16f2145f82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ae40e440c1f4a30ae2553d4038d3224":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_79b14f22a4224188bda4f9500b7607a5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4ddaed9ca49b4ba0b68e16a150c48c48","IPY_MODEL_1e306226658b4623bc83084191a00662"]}},"79b14f22a4224188bda4f9500b7607a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ddaed9ca49b4ba0b68e16a150c48c48":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_87790ed8685642a887e7be02931e80dc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":892728632,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":892728632,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a0f75124a364a76b664239d054ee337"}},"1e306226658b4623bc83084191a00662":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_db4b2ecdce094ccb9c4f06805568f7b9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 893M/893M [00:26&lt;00:00, 34.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e4dea7cd8d0416d98c642558a34bda0"}},"87790ed8685642a887e7be02931e80dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0a0f75124a364a76b664239d054ee337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db4b2ecdce094ccb9c4f06805568f7b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5e4dea7cd8d0416d98c642558a34bda0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"WIeVLU-wWk4E"},"source":["# ALBERT Fine-Tuning for Quora Dataset\n","\n","Switching from BERT to ALBERT won't require much changes relative to the notebook with BERT for sequence classification.\n","\n","There are only two code changes required to switch the code from using BERT to ALBERT:\n","1. From the HuggingFace `transformers` library, we've replaced the classes:\n","    *  `BertTokenizer` --> `AlbertTokenizer`\n","    * `BertForSequenceClassification` --> `AlbertForSequenceClassification`\n","2. For our pre-trained model, we have replaced `\"bert-base-uncased\"` with `\"albert-base-v1\"`."]},{"cell_type":"code","metadata":{"id":"GGFwTwN0T1rM"},"source":["import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mK0gnt3MS_Tn","executionInfo":{"status":"ok","timestamp":1620621438334,"user_tz":-180,"elapsed":22660,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"11deae78-8ea3-4f4b-b2dc-038b5a505a50"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lPRdDlacUASa","executionInfo":{"status":"ok","timestamp":1620621457015,"user_tz":-180,"elapsed":6248,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"f934d938-e6ba-4af5-8c5c-cef7155006e6"},"source":["df = pd.read_csv(\"gdrive/MyDrive/train.csv.zip\", compression='zip', low_memory=False)\n","print(f\"Number of texts: {df.shape[0]}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of texts: 1306122\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"96J8U9hfM-Cc"},"source":["Training on so much data would take up too much time. Hence, we're going to train only on the 10% of the original dataset. As we will see that would be enough to get really good performance measure.\n","\n","As we're dealing with skewed classes in our dataset we perform stratified splitting."]},{"cell_type":"code","metadata":{"id":"wsMn9s6wUKrB"},"source":["df, test_df = train_test_split(df, random_state=42, train_size=.1, stratify=df.target.values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"HFce0crEUX7f","executionInfo":{"status":"ok","timestamp":1620621458886,"user_tz":-180,"elapsed":1471,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"257e5bd6-907c-4445-9582-b3bb92997057"},"source":["df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qid</th>\n","      <th>question_text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>104651</th>\n","      <td>147ea801de098a0e692f</td>\n","      <td>If we trade in hourly timefram how we can pred...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>416131</th>\n","      <td>518d27683385952ea3b6</td>\n","      <td>Is there any testing or coaching that helps pe...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1218668</th>\n","      <td>eed982cc6e78e2b7dfd9</td>\n","      <td>What is Norton 360 useful for?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>341531</th>\n","      <td>42e655c6a196c459c0d0</td>\n","      <td>Cell wall of fungi made up of which substance?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1056479</th>\n","      <td>cf03b5238f820d580187</td>\n","      <td>As a parent, which martial arts class would yo...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          qid  ... target\n","104651   147ea801de098a0e692f  ...      0\n","416131   518d27683385952ea3b6  ...      0\n","1218668  eed982cc6e78e2b7dfd9  ...      0\n","341531   42e655c6a196c459c0d0  ...      0\n","1056479  cf03b5238f820d580187  ...      0\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"7wu0DawAXLaN"},"source":["In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQHzdn3zUq7O","executionInfo":{"status":"ok","timestamp":1620621464443,"user_tz":-180,"elapsed":1970,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"8bffadbf-d492-4e57-9dfc-3cd047a3b5e2"},"source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fziUP67bUz_V","executionInfo":{"status":"ok","timestamp":1620621474910,"user_tz":-180,"elapsed":6585,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"8ee03207-3dea-4f12-e5f1-10888d65f3d0"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 7.7MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 37.0MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 56.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NMr2uRU_U2wH"},"source":["sentences = df.question_text.values\n","labels = df.target.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYU_2RVnVTJH","executionInfo":{"status":"ok","timestamp":1620621481375,"user_tz":-180,"elapsed":3597,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"d1d55aec-517c-435c-d4d1-cd1fd22930cc"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\r\u001b[K     |▎                               | 10kB 28.9MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 16.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 9.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 7.4MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 7.4MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":132,"referenced_widgets":["8a68029c4215447ab3cd44404ee6f218","bb3455c16ac54cb88c0b50066c3b7f15","c40d7e8d87fb46f3b1270f55afa02c2c","a8b9048ca23948558ea714eeb8274aca","cb37b31af8424e4da46fadb3d68dbf10","27d8c380204e4be88dc53f1ef2af7a53","c7523e3922ad44c2afb39d4b2b3a7e98","eb28c862524c4a88842cf632cfd961ea","881c00c2cbf647c6b40c87dca8af3fb0","e11bc33450a14bb68ed0cb7a95bca433","268fc0428a4e4cdb83429e2a858fe844","ceb65a0a048845208c65668092ff8c0b","875d65445d854d3d8edb49b6ca0fdf6b","798013c680c2402ebffb91eccd7c31ec","79a0b25175ef45e0b39b48b5a0dc5952","76ad480b73304d10aac48da2b583a96c"]},"id":"xf7wFE8hVLDf","executionInfo":{"status":"ok","timestamp":1620621486735,"user_tz":-180,"elapsed":3189,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"c20c4a13-dc5e-4eb5-a4e7-97303ce4a7ee"},"source":["from transformers import AlbertTokenizer\n","\n","# Load the ALBERT tokenizer.\n","print('Loading ALBERT tokenizer...')\n","tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v1')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading ALBERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a68029c4215447ab3cd44404ee6f218","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"881c00c2cbf647c6b40c87dca8af3fb0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1312669.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXb5U_1vVQ4R","executionInfo":{"status":"ok","timestamp":1620540294647,"user_tz":-180,"elapsed":15901,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"a35879fb-b2cc-448a-9ebd-3f001b7ee16b"},"source":["# Print the original sentence.\n","print(' Original: ', sentences[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" Original:  If we trade in hourly timefram how we can predict what happen in 15 minutes timefram?\n","Tokenized:  ['▁if', '▁we', '▁trade', '▁in', '▁hour', 'ly', '▁time', 'fra', 'm', '▁how', '▁we', '▁can', '▁predict', '▁what', '▁happen', '▁in', '▁15', '▁minutes', '▁time', 'fra', 'm', '?']\n","Token IDs:  [100, 95, 1238, 19, 1671, 102, 85, 8691, 79, 184, 95, 92, 9584, 98, 2384, 19, 357, 902, 85, 8691, 79, 60]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6BfEkQuVod3","executionInfo":{"status":"ok","timestamp":1620540322373,"user_tz":-180,"elapsed":43616,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"42161f70-4136-4ebc-e8c9-0f3d3f1cbb06"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in tqdm(sentences):\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","\n","                        # This function also supports truncation and conversion\n","                        # to pytorch tensors, but we need to do padding, so we\n","                        # can't use these features :( .\n","                        #max_length = 128,          # Truncate all sentences.\n","                        #return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_sent)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 130612/130612 [00:28<00:00, 4643.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Original:  If we trade in hourly timefram how we can predict what happen in 15 minutes timefram?\n","Token IDs: [2, 100, 95, 1238, 19, 1671, 102, 85, 8691, 79, 184, 95, 92, 9584, 98, 2384, 19, 357, 902, 85, 8691, 79, 60, 3]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9z_Fry5-Vs2Y","executionInfo":{"status":"ok","timestamp":1620540322374,"user_tz":-180,"elapsed":43606,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"207e6afd-55c9-4f1b-de58-59a1672752a0"},"source":["print('Max sentence length: ', max([len(sen) for sen in input_ids]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max sentence length:  178\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhaooacRWSI1","executionInfo":{"status":"ok","timestamp":1620540323012,"user_tz":-180,"elapsed":44234,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"3c76d0d6-0289-4943-8cac-98a477c70c84"},"source":["# Set the maximum sequence length.\n","# I've chosen 128 for speed\n","\n","MAX_LEN = 128\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","\n","print('\\nDone.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 128 values...\n","\n","Padding token: \"<pad>\", ID: 0\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"it8vIo7_Whye","executionInfo":{"status":"ok","timestamp":1620540332740,"user_tz":-180,"elapsed":53951,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"8710e7ff-62c6-4270-e0b2-e1026f287b95"},"source":["# Create attention masks\n","attention_masks = []\n","\n","# For each sentence...\n","for sent in tqdm(input_ids):\n","    \n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 130612/130612 [00:09<00:00, 13318.48it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BismjOC9X4mu"},"source":["# Use 90% for training and 10% for validation.\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2021, test_size=0.05)\n","# Do the same for the masks.\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n","                                             random_state=2021, test_size=0.05)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cFPgy5WYtk1"},"source":["train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjkIbvQiZAkk"},"source":["batch_size = 16\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PR-OKqv8ZLBM"},"source":["## ALBERT for sequence classification\n","\n","*Note: This section has been revised for ALBERT.*\n","\n","For this task, we first want to modify the pre-trained ALBERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. \n","\n","Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of the same trained ALBERT model, each has different top layers and output types designed to accomodate their specific NLP task.  \n","\n","Here is the current list of classes provided for fine-tuning:\n","* AlbertForMaskedLM\n","* **AlbertForSequenceClassification** - The one we'll use.\n","* AlbertForQuestionAnswering\n","\n","The documentation for these can be found under [here](https://huggingface.co/transformers/model_doc/albert.html)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":883,"referenced_widgets":["ea1b3289e98c4d7aa278ce2c32929c4a","a1b07eec7de24cb28700c500e493fe85","0eb3b75c8839494d810d87b616ca9836","9b9e2d2cc3304787936ca57feef1f68c","f4960a109f7149df872a1c3e656d85fe","521b2486bd8d4a4c955c9dc4a5f3c90f","7376fde0bbad42c0b13a620a7e455847","6e332a4287d4411dac221e16f2145f82","7ae40e440c1f4a30ae2553d4038d3224","79b14f22a4224188bda4f9500b7607a5","4ddaed9ca49b4ba0b68e16a150c48c48","1e306226658b4623bc83084191a00662","87790ed8685642a887e7be02931e80dc","0a0f75124a364a76b664239d054ee337","db4b2ecdce094ccb9c4f06805568f7b9","5e4dea7cd8d0416d98c642558a34bda0"]},"id":"WJ_o8NRwZBf5","executionInfo":{"status":"ok","timestamp":1620621649862,"user_tz":-180,"elapsed":39466,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"2f09ac33-3c2c-4a30-96f3-49df8d9bc3fb"},"source":["from transformers import AlbertForSequenceClassification, AdamW, AlbertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = AlbertForSequenceClassification.from_pretrained(\n","    \"albert-xxlarge-v1\", # Using the base model, designed to be the same size as\n","                      # the original BERT-base.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea1b3289e98c4d7aa278ce2c32929c4a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=706.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ae40e440c1f4a30ae2553d4038d3224","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=892728632.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at albert-xxlarge-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["AlbertForSequenceClassification(\n","  (albert): AlbertModel(\n","    (embeddings): AlbertEmbeddings(\n","      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n","      (position_embeddings): Embedding(512, 128)\n","      (token_type_embeddings): Embedding(2, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0, inplace=False)\n","    )\n","    (encoder): AlbertTransformer(\n","      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=4096, bias=True)\n","      (albert_layer_groups): ModuleList(\n","        (0): AlbertLayerGroup(\n","          (albert_layers): ModuleList(\n","            (0): AlbertLayer(\n","              (full_layer_layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n","              (attention): AlbertAttention(\n","                (query): Linear(in_features=4096, out_features=4096, bias=True)\n","                (key): Linear(in_features=4096, out_features=4096, bias=True)\n","                (value): Linear(in_features=4096, out_features=4096, bias=True)\n","                (attention_dropout): Dropout(p=0, inplace=False)\n","                (output_dropout): Dropout(p=0, inplace=False)\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","                (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n","              )\n","              (ffn): Linear(in_features=4096, out_features=16384, bias=True)\n","              (ffn_output): Linear(in_features=16384, out_features=4096, bias=True)\n","              (dropout): Dropout(p=0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (pooler): Linear(in_features=4096, out_features=4096, bias=True)\n","    (pooler_activation): Tanh()\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=4096, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"CvoUGQpQZb0d"},"source":["Just for curiosity's sake, we can browse all of the model's parameters by name here.\n","\n","In the below cell, I've printed out the names and dimensions of the weights for:\n","\n","1. The embedding layer.\n","2. The transformer encoder layer. \n","    * In ALBERT, this layer is replicated 12 times, with the same weights used in every layer!\n","3. The output layer."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObRH1JiIZS9b","executionInfo":{"status":"ok","timestamp":1620621649863,"user_tz":-180,"elapsed":35191,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"558addb8-092e-4b78-91ad-4a305ceeee00"},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The ALBERT-base model has only {:} different named parameters.\\n'.format(len(params)))\n","\n","total_params = 0\n","\n","# For each parameter...\n","for i in range(0, len(params)):\n","    \n","    # Look up its name and size.\n","    p_name = params[i][0]\n","    p_size = params[i][1].size()\n","\n","    # Tally up the total number of individual weights.\n","    num_elements = params[i][1].numel()\n","    total_params += num_elements\n","\n","    # Print section headers between the three groups.\n","    if i == 0:\n","        print('==== Embedding Layer ====\\n')\n","    elif i == 7:\n","        print('\\n==== Transformer Encoder ====\\n')\n","    elif i == 23:\n","        print('\\n==== Output Layer ====\\n')\n","\n","    # Print out the parameter's index, name, and size.\n","    print(\"{:>2}. {:<82} {:>12}\".format(i, p_name, str(p_size)))\n","\n","print('\\nALBERT-base has {:,} unique parameter values.'.format(total_params))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The ALBERT-base model has only 27 different named parameters.\n","\n","==== Embedding Layer ====\n","\n"," 0. albert.embeddings.word_embeddings.weight                                           torch.Size([30000, 128])\n"," 1. albert.embeddings.position_embeddings.weight                                       torch.Size([512, 128])\n"," 2. albert.embeddings.token_type_embeddings.weight                                     torch.Size([2, 128])\n"," 3. albert.embeddings.LayerNorm.weight                                                 torch.Size([128])\n"," 4. albert.embeddings.LayerNorm.bias                                                   torch.Size([128])\n"," 5. albert.encoder.embedding_hidden_mapping_in.weight                                  torch.Size([4096, 128])\n"," 6. albert.encoder.embedding_hidden_mapping_in.bias                                    torch.Size([4096])\n","\n","==== Transformer Encoder ====\n","\n"," 7. albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight  torch.Size([4096])\n"," 8. albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias    torch.Size([4096])\n"," 9. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight        torch.Size([4096, 4096])\n","10. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias          torch.Size([4096])\n","11. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight          torch.Size([4096, 4096])\n","12. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias            torch.Size([4096])\n","13. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight        torch.Size([4096, 4096])\n","14. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias          torch.Size([4096])\n","15. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight        torch.Size([4096, 4096])\n","16. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias          torch.Size([4096])\n","17. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight    torch.Size([4096])\n","18. albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias      torch.Size([4096])\n","19. albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight                    torch.Size([16384, 4096])\n","20. albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias                      torch.Size([16384])\n","21. albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight             torch.Size([4096, 16384])\n","22. albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias               torch.Size([4096])\n","\n","==== Output Layer ====\n","\n","23. albert.pooler.weight                                                               torch.Size([4096, 4096])\n","24. albert.pooler.bias                                                                 torch.Size([4096])\n","25. classifier.weight                                                                  torch.Size([2, 4096])\n","26. classifier.bias                                                                    torch.Size([2])\n","\n","ALBERT-base has 222,603,778 unique parameter values.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"juJq6KIzZep3"},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 1e-5, # From run_glue.sh\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3mdfhAuaWlY"},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs\n","epochs = 1 # Estimated based on 5,336 training steps in run_glue.sh\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 320, # From run_glue.sh\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pv7qkSrHaYhy"},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBKmMl2QadY-"},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hhx5u0ddafRH","executionInfo":{"status":"ok","timestamp":1620570894465,"user_tz":-180,"elapsed":30615562,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"87271818-aa30-4402-ae9c-510f092d479c"},"source":["import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 1 ========\n","Training...\n","  Batch    40  of  7,756.    Elapsed: 0:02:35.\n","  Batch    80  of  7,756.    Elapsed: 0:05:10.\n","  Batch   120  of  7,756.    Elapsed: 0:07:44.\n","  Batch   160  of  7,756.    Elapsed: 0:10:19.\n","  Batch   200  of  7,756.    Elapsed: 0:12:54.\n","  Batch   240  of  7,756.    Elapsed: 0:15:29.\n","  Batch   280  of  7,756.    Elapsed: 0:18:04.\n","  Batch   320  of  7,756.    Elapsed: 0:20:38.\n","  Batch   360  of  7,756.    Elapsed: 0:23:13.\n","  Batch   400  of  7,756.    Elapsed: 0:25:48.\n","  Batch   440  of  7,756.    Elapsed: 0:28:23.\n","  Batch   480  of  7,756.    Elapsed: 0:30:58.\n","  Batch   520  of  7,756.    Elapsed: 0:33:32.\n","  Batch   560  of  7,756.    Elapsed: 0:36:07.\n","  Batch   600  of  7,756.    Elapsed: 0:38:42.\n","  Batch   640  of  7,756.    Elapsed: 0:41:17.\n","  Batch   680  of  7,756.    Elapsed: 0:43:52.\n","  Batch   720  of  7,756.    Elapsed: 0:46:26.\n","  Batch   760  of  7,756.    Elapsed: 0:49:01.\n","  Batch   800  of  7,756.    Elapsed: 0:51:36.\n","  Batch   840  of  7,756.    Elapsed: 0:54:11.\n","  Batch   880  of  7,756.    Elapsed: 0:56:46.\n","  Batch   920  of  7,756.    Elapsed: 0:59:20.\n","  Batch   960  of  7,756.    Elapsed: 1:01:55.\n","  Batch 1,000  of  7,756.    Elapsed: 1:04:30.\n","  Batch 1,040  of  7,756.    Elapsed: 1:07:05.\n","  Batch 1,080  of  7,756.    Elapsed: 1:09:40.\n","  Batch 1,120  of  7,756.    Elapsed: 1:12:14.\n","  Batch 1,160  of  7,756.    Elapsed: 1:14:49.\n","  Batch 1,200  of  7,756.    Elapsed: 1:17:24.\n","  Batch 1,240  of  7,756.    Elapsed: 1:19:59.\n","  Batch 1,280  of  7,756.    Elapsed: 1:22:33.\n","  Batch 1,320  of  7,756.    Elapsed: 1:25:08.\n","  Batch 1,360  of  7,756.    Elapsed: 1:27:43.\n","  Batch 1,400  of  7,756.    Elapsed: 1:30:18.\n","  Batch 1,440  of  7,756.    Elapsed: 1:32:53.\n","  Batch 1,480  of  7,756.    Elapsed: 1:35:27.\n","  Batch 1,520  of  7,756.    Elapsed: 1:38:02.\n","  Batch 1,560  of  7,756.    Elapsed: 1:40:37.\n","  Batch 1,600  of  7,756.    Elapsed: 1:43:12.\n","  Batch 1,640  of  7,756.    Elapsed: 1:45:47.\n","  Batch 1,680  of  7,756.    Elapsed: 1:48:21.\n","  Batch 1,720  of  7,756.    Elapsed: 1:50:56.\n","  Batch 1,760  of  7,756.    Elapsed: 1:53:31.\n","  Batch 1,800  of  7,756.    Elapsed: 1:56:06.\n","  Batch 1,840  of  7,756.    Elapsed: 1:58:40.\n","  Batch 1,880  of  7,756.    Elapsed: 2:01:15.\n","  Batch 1,920  of  7,756.    Elapsed: 2:03:50.\n","  Batch 1,960  of  7,756.    Elapsed: 2:06:25.\n","  Batch 2,000  of  7,756.    Elapsed: 2:09:00.\n","  Batch 2,040  of  7,756.    Elapsed: 2:11:34.\n","  Batch 2,080  of  7,756.    Elapsed: 2:14:09.\n","  Batch 2,120  of  7,756.    Elapsed: 2:16:44.\n","  Batch 2,160  of  7,756.    Elapsed: 2:19:19.\n","  Batch 2,200  of  7,756.    Elapsed: 2:21:54.\n","  Batch 2,240  of  7,756.    Elapsed: 2:24:28.\n","  Batch 2,280  of  7,756.    Elapsed: 2:27:03.\n","  Batch 2,320  of  7,756.    Elapsed: 2:29:38.\n","  Batch 2,360  of  7,756.    Elapsed: 2:32:13.\n","  Batch 2,400  of  7,756.    Elapsed: 2:34:48.\n","  Batch 2,440  of  7,756.    Elapsed: 2:37:22.\n","  Batch 2,480  of  7,756.    Elapsed: 2:39:57.\n","  Batch 2,520  of  7,756.    Elapsed: 2:42:32.\n","  Batch 2,560  of  7,756.    Elapsed: 2:45:07.\n","  Batch 2,600  of  7,756.    Elapsed: 2:47:42.\n","  Batch 2,640  of  7,756.    Elapsed: 2:50:16.\n","  Batch 2,680  of  7,756.    Elapsed: 2:52:51.\n","  Batch 2,720  of  7,756.    Elapsed: 2:55:26.\n","  Batch 2,760  of  7,756.    Elapsed: 2:58:01.\n","  Batch 2,800  of  7,756.    Elapsed: 3:00:35.\n","  Batch 2,840  of  7,756.    Elapsed: 3:03:10.\n","  Batch 2,880  of  7,756.    Elapsed: 3:05:45.\n","  Batch 2,920  of  7,756.    Elapsed: 3:08:20.\n","  Batch 2,960  of  7,756.    Elapsed: 3:10:55.\n","  Batch 3,000  of  7,756.    Elapsed: 3:13:29.\n","  Batch 3,040  of  7,756.    Elapsed: 3:16:04.\n","  Batch 3,080  of  7,756.    Elapsed: 3:18:39.\n","  Batch 3,120  of  7,756.    Elapsed: 3:21:14.\n","  Batch 3,160  of  7,756.    Elapsed: 3:23:48.\n","  Batch 3,200  of  7,756.    Elapsed: 3:26:23.\n","  Batch 3,240  of  7,756.    Elapsed: 3:28:58.\n","  Batch 3,280  of  7,756.    Elapsed: 3:31:33.\n","  Batch 3,320  of  7,756.    Elapsed: 3:34:08.\n","  Batch 3,360  of  7,756.    Elapsed: 3:36:42.\n","  Batch 3,400  of  7,756.    Elapsed: 3:39:17.\n","  Batch 3,440  of  7,756.    Elapsed: 3:41:52.\n","  Batch 3,480  of  7,756.    Elapsed: 3:44:27.\n","  Batch 3,520  of  7,756.    Elapsed: 3:47:02.\n","  Batch 3,560  of  7,756.    Elapsed: 3:49:36.\n","  Batch 3,600  of  7,756.    Elapsed: 3:52:11.\n","  Batch 3,640  of  7,756.    Elapsed: 3:54:46.\n","  Batch 3,680  of  7,756.    Elapsed: 3:57:21.\n","  Batch 3,720  of  7,756.    Elapsed: 3:59:55.\n","  Batch 3,760  of  7,756.    Elapsed: 4:02:30.\n","  Batch 3,800  of  7,756.    Elapsed: 4:05:05.\n","  Batch 3,840  of  7,756.    Elapsed: 4:07:40.\n","  Batch 3,880  of  7,756.    Elapsed: 4:10:15.\n","  Batch 3,920  of  7,756.    Elapsed: 4:12:49.\n","  Batch 3,960  of  7,756.    Elapsed: 4:15:24.\n","  Batch 4,000  of  7,756.    Elapsed: 4:17:59.\n","  Batch 4,040  of  7,756.    Elapsed: 4:20:34.\n","  Batch 4,080  of  7,756.    Elapsed: 4:23:09.\n","  Batch 4,120  of  7,756.    Elapsed: 4:25:43.\n","  Batch 4,160  of  7,756.    Elapsed: 4:28:18.\n","  Batch 4,200  of  7,756.    Elapsed: 4:30:53.\n","  Batch 4,240  of  7,756.    Elapsed: 4:33:28.\n","  Batch 4,280  of  7,756.    Elapsed: 4:36:03.\n","  Batch 4,320  of  7,756.    Elapsed: 4:38:37.\n","  Batch 4,360  of  7,756.    Elapsed: 4:41:12.\n","  Batch 4,400  of  7,756.    Elapsed: 4:43:47.\n","  Batch 4,440  of  7,756.    Elapsed: 4:46:22.\n","  Batch 4,480  of  7,756.    Elapsed: 4:48:56.\n","  Batch 4,520  of  7,756.    Elapsed: 4:51:31.\n","  Batch 4,560  of  7,756.    Elapsed: 4:54:06.\n","  Batch 4,600  of  7,756.    Elapsed: 4:56:41.\n","  Batch 4,640  of  7,756.    Elapsed: 4:59:16.\n","  Batch 4,680  of  7,756.    Elapsed: 5:01:50.\n","  Batch 4,720  of  7,756.    Elapsed: 5:04:25.\n","  Batch 4,760  of  7,756.    Elapsed: 5:07:00.\n","  Batch 4,800  of  7,756.    Elapsed: 5:09:35.\n","  Batch 4,840  of  7,756.    Elapsed: 5:12:10.\n","  Batch 4,880  of  7,756.    Elapsed: 5:14:44.\n","  Batch 4,920  of  7,756.    Elapsed: 5:17:19.\n","  Batch 4,960  of  7,756.    Elapsed: 5:19:54.\n","  Batch 5,000  of  7,756.    Elapsed: 5:22:29.\n","  Batch 5,040  of  7,756.    Elapsed: 5:25:03.\n","  Batch 5,080  of  7,756.    Elapsed: 5:27:38.\n","  Batch 5,120  of  7,756.    Elapsed: 5:30:13.\n","  Batch 5,160  of  7,756.    Elapsed: 5:32:48.\n","  Batch 5,200  of  7,756.    Elapsed: 5:35:23.\n","  Batch 5,240  of  7,756.    Elapsed: 5:37:57.\n","  Batch 5,280  of  7,756.    Elapsed: 5:40:32.\n","  Batch 5,320  of  7,756.    Elapsed: 5:43:07.\n","  Batch 5,360  of  7,756.    Elapsed: 5:45:42.\n","  Batch 5,400  of  7,756.    Elapsed: 5:48:17.\n","  Batch 5,440  of  7,756.    Elapsed: 5:50:51.\n","  Batch 5,480  of  7,756.    Elapsed: 5:53:26.\n","  Batch 5,520  of  7,756.    Elapsed: 5:56:01.\n","  Batch 5,560  of  7,756.    Elapsed: 5:58:36.\n","  Batch 5,600  of  7,756.    Elapsed: 6:01:11.\n","  Batch 5,640  of  7,756.    Elapsed: 6:03:45.\n","  Batch 5,680  of  7,756.    Elapsed: 6:06:20.\n","  Batch 5,720  of  7,756.    Elapsed: 6:08:55.\n","  Batch 5,760  of  7,756.    Elapsed: 6:11:30.\n","  Batch 5,800  of  7,756.    Elapsed: 6:14:05.\n","  Batch 5,840  of  7,756.    Elapsed: 6:16:39.\n","  Batch 5,880  of  7,756.    Elapsed: 6:19:14.\n","  Batch 5,920  of  7,756.    Elapsed: 6:21:49.\n","  Batch 5,960  of  7,756.    Elapsed: 6:24:24.\n","  Batch 6,000  of  7,756.    Elapsed: 6:26:59.\n","  Batch 6,040  of  7,756.    Elapsed: 6:29:33.\n","  Batch 6,080  of  7,756.    Elapsed: 6:32:08.\n","  Batch 6,120  of  7,756.    Elapsed: 6:34:43.\n","  Batch 6,160  of  7,756.    Elapsed: 6:37:18.\n","  Batch 6,200  of  7,756.    Elapsed: 6:39:53.\n","  Batch 6,240  of  7,756.    Elapsed: 6:42:27.\n","  Batch 6,280  of  7,756.    Elapsed: 6:45:02.\n","  Batch 6,320  of  7,756.    Elapsed: 6:47:37.\n","  Batch 6,360  of  7,756.    Elapsed: 6:50:12.\n","  Batch 6,400  of  7,756.    Elapsed: 6:52:47.\n","  Batch 6,440  of  7,756.    Elapsed: 6:55:21.\n","  Batch 6,480  of  7,756.    Elapsed: 6:57:56.\n","  Batch 6,520  of  7,756.    Elapsed: 7:00:31.\n","  Batch 6,560  of  7,756.    Elapsed: 7:03:06.\n","  Batch 6,600  of  7,756.    Elapsed: 7:05:41.\n","  Batch 6,640  of  7,756.    Elapsed: 7:08:15.\n","  Batch 6,680  of  7,756.    Elapsed: 7:10:50.\n","  Batch 6,720  of  7,756.    Elapsed: 7:13:25.\n","  Batch 6,760  of  7,756.    Elapsed: 7:16:00.\n","  Batch 6,800  of  7,756.    Elapsed: 7:18:35.\n","  Batch 6,840  of  7,756.    Elapsed: 7:21:09.\n","  Batch 6,880  of  7,756.    Elapsed: 7:23:44.\n","  Batch 6,920  of  7,756.    Elapsed: 7:26:19.\n","  Batch 6,960  of  7,756.    Elapsed: 7:28:54.\n","  Batch 7,000  of  7,756.    Elapsed: 7:31:28.\n","  Batch 7,040  of  7,756.    Elapsed: 7:34:03.\n","  Batch 7,080  of  7,756.    Elapsed: 7:36:38.\n","  Batch 7,120  of  7,756.    Elapsed: 7:39:13.\n","  Batch 7,160  of  7,756.    Elapsed: 7:41:48.\n","  Batch 7,200  of  7,756.    Elapsed: 7:44:22.\n","  Batch 7,240  of  7,756.    Elapsed: 7:46:57.\n","  Batch 7,280  of  7,756.    Elapsed: 7:49:32.\n","  Batch 7,320  of  7,756.    Elapsed: 7:52:07.\n","  Batch 7,360  of  7,756.    Elapsed: 7:54:42.\n","  Batch 7,400  of  7,756.    Elapsed: 7:57:17.\n","  Batch 7,440  of  7,756.    Elapsed: 7:59:51.\n","  Batch 7,480  of  7,756.    Elapsed: 8:02:26.\n","  Batch 7,520  of  7,756.    Elapsed: 8:05:01.\n","  Batch 7,560  of  7,756.    Elapsed: 8:07:36.\n","  Batch 7,600  of  7,756.    Elapsed: 8:10:11.\n","  Batch 7,640  of  7,756.    Elapsed: 8:12:45.\n","  Batch 7,680  of  7,756.    Elapsed: 8:15:20.\n","  Batch 7,720  of  7,756.    Elapsed: 8:17:55.\n","\n","  Average training loss: 0.12\n","  Training epcoh took: 8:20:11\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation took: 0:08:59\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qqye57c9k6hL","executionInfo":{"status":"ok","timestamp":1620621900987,"user_tz":-180,"elapsed":17762,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"2ad18743-73db-4704-c63e-55ae52bf3f7e"},"source":["saved_dir = './gdrive/MyDrive/model_save/'\n","\n","trained_model = AlbertForSequenceClassification.from_pretrained(\n","    saved_dir, # Using the base model, designed to be the same size as\n","                      # the original BERT-base.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","trained_model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlbertForSequenceClassification(\n","  (albert): AlbertModel(\n","    (embeddings): AlbertEmbeddings(\n","      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n","      (position_embeddings): Embedding(512, 128)\n","      (token_type_embeddings): Embedding(2, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0, inplace=False)\n","    )\n","    (encoder): AlbertTransformer(\n","      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=4096, bias=True)\n","      (albert_layer_groups): ModuleList(\n","        (0): AlbertLayerGroup(\n","          (albert_layers): ModuleList(\n","            (0): AlbertLayer(\n","              (full_layer_layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n","              (attention): AlbertAttention(\n","                (query): Linear(in_features=4096, out_features=4096, bias=True)\n","                (key): Linear(in_features=4096, out_features=4096, bias=True)\n","                (value): Linear(in_features=4096, out_features=4096, bias=True)\n","                (attention_dropout): Dropout(p=0, inplace=False)\n","                (output_dropout): Dropout(p=0, inplace=False)\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","                (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n","              )\n","              (ffn): Linear(in_features=4096, out_features=16384, bias=True)\n","              (ffn_output): Linear(in_features=16384, out_features=4096, bias=True)\n","              (dropout): Dropout(p=0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (pooler): Linear(in_features=4096, out_features=4096, bias=True)\n","    (pooler_activation): Tanh()\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=4096, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"mZRNBpRhmWSY","executionInfo":{"status":"ok","timestamp":1620571887140,"user_tz":-180,"elapsed":1073,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"6417319f-264c-430d-c233-6181c57f0337"},"source":["test_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qid</th>\n","      <th>question_text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>452819</th>\n","      <td>58b326e49fa20fe99ea6</td>\n","      <td>What could be the very last thought of a perso...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1193005</th>\n","      <td>e9d1086f367ec17a0b91</td>\n","      <td>How can I work on the impact of digital securi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>164018</th>\n","      <td>2012196811a3046d33b5</td>\n","      <td>16. Let X and Y be independent and identically...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>784867</th>\n","      <td>99c1082398f2165e0fbe</td>\n","      <td>Why does the Indian Muslim can't marry Europea...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>135624</th>\n","      <td>1a8ccde5c1e68910a27f</td>\n","      <td>How GST will affect life of farmer?</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          qid  ... target\n","452819   58b326e49fa20fe99ea6  ...      0\n","1193005  e9d1086f367ec17a0b91  ...      0\n","164018   2012196811a3046d33b5  ...      0\n","784867   99c1082398f2165e0fbe  ...      0\n","135624   1a8ccde5c1e68910a27f  ...      0\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gq7V4Ep7nJFi","executionInfo":{"status":"ok","timestamp":1620632502545,"user_tz":-180,"elapsed":1838,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"384493ae-4f15-4932-8e66-d3b428ee1859"},"source":["df, _ = train_test_split(test_df, random_state=42, train_size=.1, stratify=test_df.target.values)\n","df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(117551, 3)"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"JV58Cj5SoVX4"},"source":["sentences = df.question_text.values\n","labels = df.target.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SM3aZLl_ogKI","executionInfo":{"status":"ok","timestamp":1620632528067,"user_tz":-180,"elapsed":25294,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"f12cafc8-c86c-42f8-eb93-f1c2fbccf539"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in tqdm(sentences):\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","\n","                        # This function also supports truncation and conversion\n","                        # to pytorch tensors, but we need to do padding, so we\n","                        # can't use these features :( .\n","                        #max_length = 128,          # Truncate all sentences.\n","                        #return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 117551/117551 [00:24<00:00, 4817.71it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LtoNP6hor0L","executionInfo":{"status":"ok","timestamp":1620632537195,"user_tz":-180,"elapsed":32292,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"074a8eb8-0bb2-48d0-ffcb-d7b9a9d7844e"},"source":["MAX_LEN = 128\n","\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# For each sentence...\n","for sent in tqdm(input_ids):\n","    \n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 117551/117551 [00:08<00:00, 13953.62it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YOLRzX5Wo1Ta"},"source":["test_inputs = torch.tensor(input_ids)\n","\n","test_labels = torch.tensor(labels)\n","\n","test_masks = torch.tensor(attention_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sIAY2y_8pVfx"},"source":["batch_size = 16\n","\n","# Create the DataLoader for our validation set.\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaB3hAE1p92k"},"source":["from sklearn.metrics import f1_score\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iDRjdfxuqDKP","executionInfo":{"status":"ok","timestamp":1620642367576,"user_tz":-180,"elapsed":9830340,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"f3728683-8f7d-4c33-dfa2-a21d0c0355d4"},"source":["trained_model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in tqdm(test_dataloader):\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = trained_model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 7347/7347 [2:43:48<00:00,  1.34s/it]"],"name":"stderr"},{"output_type":"stream","text":["    DONE.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eSvlPJPyqU7G"},"source":["predictions = np.concatenate(predictions)\n","true_labels = np.concatenate(true_labels)\n","# f1_score(predictions, true_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlNWuggjruhf","executionInfo":{"status":"ok","timestamp":1620642367584,"user_tz":-180,"elapsed":9830256,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"01bb794e-0bfe-4575-ae23-a702f98c5936"},"source":["f1_score(true_labels, np.argmax(predictions, axis=1))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7071869736103312"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"meYgYcoHr58S","executionInfo":{"status":"ok","timestamp":1620573176704,"user_tz":-180,"elapsed":9403,"user":{"displayName":"Константин Могилевский","photoUrl":"","userId":"08319258411892990578"}},"outputId":"a9b75710-bb77-449d-f36d-2b097cd9fe2d"},"source":["output_dir = './model_save/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(f\"Saving model to {output_dir}\")\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n","\n","# Mount Google Drive to this Notebook instance.\n","drive.mount('/content/drive')\n","\n","# Copy the model files to a directory in your Google Drive.\n","!cp -r ./model_save/ \"/content/drive/MyDrive\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model to ./model_save/\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J4hCjtQ6sB5Z"},"source":[""],"execution_count":null,"outputs":[]}]}