# Natural Language Processing projects (Usage of Transformer-based architectures)

| Name of the project | Description | Used Libraries | 
| :---------------------- | :---------------------- | :---------------------- |
| [ALBERT for text classification](ALBERT_xxlarge_Quora_Dataset) | This is just a simple usage of the ALBERT model for text classification on Quora Dataset. This approach is very similar to text classification with BERT, where I describe more thoroughly every step. Training such a model only on 20% of the data took up about 7 hours and achieved really great performance measure compared to LeaderBoard on Kaggle | *os*, *keras*, *sklearn*, *matplotlib.pyplot*, *pytorch*, *transformers*. |
| [BERT-large for sentiment analysis](BERT_large_SST2) | In this notebook we use HuggingFace PyTorch library to quickly fine-tune the model based on BERT-large and to get near state of the art performance on the problem that is included in the GLUE benchmark: SST2. | *transformers*, *os*, *time*, *torch*, *random*, *datetime*, *numpy*, *pandas*, *seaborn*, *matplotlib*, *sklearn*, *keras*. |
| [BERT-base for multi-label text classification](BERT_multi_label_task) | This notebook shows how to fine-tune BERT-base for multi-label text classification on Wiki Comments Dataset. This is a slightly different task than "multiclass" classification, where there are multiple categories, but each sample only belongs to one category. The huggingface transformers library has built-in support for multiclass, but not multi-label, so we'll be defining a custom class here to do it! | *transformers*, *os*, *time*, *torch*, *random*, *datetime*, *numpy*, *pandas*, *seaborn*, *matplotlib*, *sklearn*, *keras*. |
| [BERT for Question Answering](BERT_for_QA) | For something like text classification, you definitely want to fine-tune BERT on your own dataset. For question answering, however, it seems like you may be able to get decent results using a model that's already been fine-tuned on the SQuAD benchmark. In this Notebook, we'll do exactly that, and see that it performs well on text that wasn't in the SQuAD dataset. | *transformers*, *torch*, *seaborn*, *matplotlib*, *BertTokenizer*, *BertForQuestionAnswering*. |
| [distilBERT for text classification and semantic extraction](distilBERT) | This Notebook shows you how to fine-tune distilBERT for document classification tasks using the Wikipedia Personal Attacks dataset as an example. As a bonus, we'll also look briefly at how we can apply BERT to search for "semantically similar" comments in the dataset. | *transformers*, *os*, *time*, *torch*, *random*, *datetime*, *numpy*, *pandas*, *seaborn*, *matplotlib*, *sklearn*, *keras*, *BertTokenizer*, *BertForSequenceClassification*. |
| [Fine-tuning BERT for text classification on Quora Dataset using TensorFlow and TF-Hub](BERT_base_tfhub) | This notebook shows the same usage of BERT-base for text classification but via different library. | *os*, *numpy*, *pandas*, *tensorflow*, *tensorflow_hub*, *sklearn*, *matplotlib.pyplot*, *official.nlp*. |
| [Multilingul model approach for dealing with non-english text](XLM-R_Arabic) | Here we're trying to perform a text classification on Arabic toxic comments with fine-tuning XLM-R model on English dataset. Surprisingly, this approach works really good. | *os*, *numpy*, *pandas*, *csv*, *nltk*, *time*, *datetime*, *transformers*, *tqdm*, *torch*, *XLMRobertaTokenizer*, *XLMRobertaForSequenceClassification*. |
| [Language model and Machine Translation with RNN](seq2seq) | Scripts in this folder implements *Language Model*(simply predicting next word in a sentence based on the previous ones) and *Seq2Seq architecture*, where Encoder is a RNN with Bidirectional LSTMs and Decoder is just a RNN with LSTMs. At first, we use this architecture for Machine Translation without attention mechanism and then with it, which improves quality of a translation. | *os*, *sys*, *keras*, *sklearn*, *matplotlib.pyplot*. |
